{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from src.dataset_info import get_dataset_info\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.utils import token_segments, text_ft_index_ends\n",
    "\n",
    "# from src.models import Model\n",
    "import lightgbm as lgb\n",
    "from src.models import WeightedEnsemble, StackModel, AllAsTextModel\n",
    "from src.joint_masker import JointMasker\n",
    "import argparse\n",
    "import scipy as sp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_type, ds_type, test_set_size=100, tab_scale_factor=2):\n",
    "    di = get_dataset_info(ds_type, model_type)\n",
    "    # Data\n",
    "    train_df = load_dataset(\n",
    "        di.ds_name, split=\"train\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name, split=\"test\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df_sample = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if model_type == \"all_text\":\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            # model=di.text_model_name,\n",
    "            model=\"../models/wine/glowing-morning-9/checkpoint-6705\",\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert all columns to a single string\n",
    "\n",
    "        def cols_to_str_fn(array):\n",
    "            return \" | \".join(\n",
    "                [f\"{col}: {val}\" for col, val in zip(\n",
    "                    di.tab_cols + di.text_cols, array)]\n",
    "            )\n",
    "\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "            # cols=di.tab_cols + di.text_cols\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(di.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(di.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\n",
    "            \"category\")\n",
    "        test_df_sample[di.categorical_cols] = test_df_sample[\n",
    "            di.categorical_cols\n",
    "        ].astype(\"category\")\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42)\n",
    "        tab_model.fit(train_df[di.tab_cols], y_train)\n",
    "\n",
    "        if model_type == \"ensemble_50\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.5,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_75\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.75,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_25\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.25,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                di.ds_name, split=\"validation\", download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\n",
    "                \"category\")\n",
    "            y_val = val_df[di.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            # text_val_preds = np.array(\n",
    "            #     [format_text_pred(pred) for pred in text_val_preds]\n",
    "            # )\n",
    "            text_val_preds = np.array(\n",
    "                [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[di.tab_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(stack_val_df)\n",
    "            for i in range(text_val_preds.shape[1]):\n",
    "                stack_val_df[f\"text_pred_{i}\"] = text_val_preds[:, i]\n",
    "            for i in range(tab_val_preds.shape[1]):\n",
    "                stack_val_df[f\"tab_pred_{i}\"] = tab_val_preds[:, i]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(random_state=42)\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    test_sample_vals = test_df_sample[di.tab_cols + di.text_cols].values\n",
    "    test_vals = test_df[di.tab_cols + di.text_cols].values\n",
    "\n",
    "    return (\n",
    "        model.predict(test_sample_vals),\n",
    "        test_df_sample[di.label_col].values,\n",
    "        model.predict(test_vals),\n",
    "        test_df[di.label_col].values,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Job Postings (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 90825.12it/s]\n",
      "3182it [00:00, 54504.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on fake\n",
      "Test accuaracy: 0.9387177875549969\n",
      "Test accuaracy (sample): 0.93\n",
      "Test AUC: 0.8744072444723762\n",
      "Test AUC (sample): 0.808421052631579\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 97792.12it/s]\n",
      "3182it [00:00, 53235.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on fake\n",
      "Test accuaracy: 0.9619736015084852\n",
      "Test accuaracy (sample): 0.96\n",
      "Test AUC: 0.9189305642842179\n",
      "Test AUC (sample): 0.9136842105263159\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 95847.90it/s]\n",
      "3182it [00:00, 53839.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on fake\n",
      "Test accuaracy: 0.9783155248271528\n",
      "Test accuaracy (sample): 0.99\n",
      "Test AUC: 0.9337994439048545\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_all_text-b16faba3acce3185/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_all_text-b16faba3acce3185/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_text on fake\n",
      "Test accuaracy: 0.9745443117536141\n",
      "Test accuaracy (sample): 0.99\n",
      "Test AUC: 0.9420670742158488\n",
      "Test AUC (sample): 0.9642105263157895\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 273kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 77.5MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 27.3MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 40.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2994.51it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 95108.93it/s]\n",
      "3182it [00:00, 54891.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on fake\n",
      "Test accuaracy: 0.943117536140792\n",
      "Test accuaracy (sample): 0.93\n",
      "Test AUC: 0.9087501666380999\n",
      "Test AUC (sample): 0.9557894736842105\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"fake\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB  (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 688719.87it/s]\n",
      "200it [00:00, 964207.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on imdb_genre\n",
      "Test accuaracy: 0.78\n",
      "Test accuaracy (sample): 0.79\n",
      "Test AUC: 0.8530677609848865\n",
      "Test AUC (sample): 0.8262626262626263\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 759837.68it/s]\n",
      "200it [00:00, 975419.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on imdb_genre\n",
      "Test accuaracy: 0.785\n",
      "Test accuaracy (sample): 0.79\n",
      "Test AUC: 0.8646782103893504\n",
      "Test AUC (sample): 0.8513131313131314\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 649273.07it/s]\n",
      "200it [00:00, 969781.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on imdb_genre\n",
      "Test accuaracy: 0.775\n",
      "Test accuaracy (sample): 0.75\n",
      "Test AUC: 0.8520668601741568\n",
      "Test AUC (sample): 0.8387878787878787\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_text on imdb_genre\n",
      "Test accuaracy: 0.715\n",
      "Test accuaracy (sample): 0.69\n",
      "Test AUC: 0.8042238014212791\n",
      "Test AUC (sample): 0.8036363636363636\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 880kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.24MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 4.86MB/s]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 5.38MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2481.35it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 383742.36it/s]\n",
      "200it [00:00, 735842.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on imdb_genre\n",
      "Test accuaracy: 0.74\n",
      "Test accuaracy (sample): 0.73\n",
      "Test AUC: 0.8106295666099489\n",
      "Test AUC (sample): 0.7846464646464647\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"imdb_genre\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kickstarter  (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 326kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 75.7MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 37.3MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 47.0MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2821.28it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 615/615 [00:00<00:00, 204kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:09<00:00, 28.4MB/s] \n",
      "100it [00:00, 341834.07it/s]\n",
      "21626it [00:00, 332713.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on kick\n",
      "Test accuaracy: 0.6999445112364746\n",
      "Test accuaracy (sample): 0.66\n",
      "Test AUC: 0.7410626481173135\n",
      "Test AUC (sample): 0.7507507507507508\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 437818.79it/s]\n",
      "21626it [00:00, 329554.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on kick\n",
      "Test accuaracy: 0.7136779802090076\n",
      "Test accuaracy (sample): 0.67\n",
      "Test AUC: 0.7735201424600657\n",
      "Test AUC (sample): 0.7691977691977692\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 361266.49it/s]\n",
      "21626it [00:00, 327059.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on kick\n",
      "Test accuaracy: 0.7177471562008694\n",
      "Test accuaracy (sample): 0.69\n",
      "Test AUC: 0.7679046143380706\n",
      "Test AUC (sample): 0.7301587301587301\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 615/615 [00:00<00:00, 245kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:05<00:00, 46.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_text on kick\n",
      "Test accuaracy: 0.7288911495422177\n",
      "Test accuaracy (sample): 0.66\n",
      "Test AUC: 0.7755581173243297\n",
      "Test AUC (sample): 0.7687687687687688\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 1.00MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 81.4MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 40.1MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.5MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.34it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2820.02it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 416928.83it/s]\n",
      "21626it [00:00, 326971.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on kick\n",
      "Test accuaracy: 0.7072967724035882\n",
      "Test accuaracy (sample): 0.65\n",
      "Test AUC: 0.7491010899483486\n",
      "Test AUC (sample): 0.7211497211497212\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"kick\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jigsaw (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:05<00:00, 46.1MB/s] \n",
      "100it [00:00, 437818.79it/s]\n",
      "25000it [00:00, 179864.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on jigsaw\n",
      "Test accuaracy: 0.94256\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9321638495642937\n",
      "Test AUC (sample): 0.8378947368421054\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 438276.28it/s]\n",
      "25000it [00:00, 459371.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on jigsaw\n",
      "Test accuaracy: 0.9448\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9456286008527085\n",
      "Test AUC (sample): 0.8631578947368422\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 422812.90it/s]\n",
      "25000it [00:00, 175630.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on jigsaw\n",
      "Test accuaracy: 0.9608\n",
      "Test accuaracy (sample): 0.96\n",
      "Test AUC: 0.9511750950554884\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_text on jigsaw\n",
      "Test accuaracy: 0.95148\n",
      "Test accuaracy (sample): 0.94\n",
      "Test AUC: 0.9500019147868117\n",
      "Test AUC (sample): 0.9157894736842106\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 673kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 83.9MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 41.3MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 57.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.24it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3004.52it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 353651.26it/s]\n",
      "25000it [00:00, 178293.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on jigsaw\n",
      "Test accuaracy: 0.93208\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9158202495728693\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"jigsaw\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 719434.65it/s]\n",
      "1273it [00:00, 1353790.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on prod_sent\n",
      "Test accuaracy: 0.8908091123330715\n",
      "Test accuaracy (sample): 0.95\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 719434.65it/s]\n",
      "1273it [00:00, 1319009.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on prod_sent\n",
      "Test accuaracy: 0.8695993715632364\n",
      "Test accuaracy (sample): 0.9\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100it [00:00, 932067.56it/s]\n",
      "1273it [00:00, 1058554.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on prod_sent\n",
      "Test accuaracy: 0.6614296936370778\n",
      "Test accuaracy (sample): 0.64\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_text on prod_sent\n",
      "Test accuaracy: 0.8279654359780048\n",
      "Test accuaracy (sample): 0.87\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 247kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 10.6MB/s]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 798kB/s]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.60MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3104.59it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 746317.44it/s]\n",
      "1273it [00:00, 1028181.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on prod_sent\n",
      "Test accuaracy: 0.6496465043205027\n",
      "Test accuaracy (sample): 0.68\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"prod_sent\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    # print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    # print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_type, ds_type, test_set_size=100, tab_scale_factor=2):\n",
    "    di = get_dataset_info(ds_type, model_type)\n",
    "    # Data\n",
    "    train_df = load_dataset(\n",
    "        di.ds_name,\n",
    "        split=\"train\",  # download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name,\n",
    "        split=\"test\",  # download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df_sample = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if model_type == \"all_text\":\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            # model=di.text_model_name,\n",
    "            model=\"../models/wine/glowing-morning-9/checkpoint-6705\",\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert all columns to a single string\n",
    "\n",
    "        def cols_to_str_fn(array):\n",
    "            return \" | \".join(\n",
    "                [f\"{col}: {val}\" for col, val in zip(di.tab_cols + di.text_cols, array)]\n",
    "            )\n",
    "\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "            # cols=di.tab_cols + di.text_cols\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(di.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(di.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\"category\")\n",
    "        test_df_sample[di.categorical_cols] = test_df_sample[\n",
    "            di.categorical_cols\n",
    "        ].astype(\"category\")\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42)\n",
    "        tab_model.fit(train_df[di.tab_cols], y_train)\n",
    "\n",
    "        if model_type == \"ensemble_50\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.5,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_75\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.75,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_25\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.25,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                di.ds_name, split=\"validation\", download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\"category\")\n",
    "            y_val = val_df[di.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            # text_val_preds = np.array(\n",
    "            #     [format_text_pred(pred) for pred in text_val_preds]\n",
    "            # )\n",
    "            text_val_preds = np.array(\n",
    "                [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[di.tab_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(stack_val_df)\n",
    "            for i in range(text_val_preds.shape[1]):\n",
    "                stack_val_df[f\"text_pred_{i}\"] = text_val_preds[:, i]\n",
    "            for i in range(tab_val_preds.shape[1]):\n",
    "                stack_val_df[f\"tab_pred_{i}\"] = tab_val_preds[:, i]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(random_state=42)\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    test_sample_vals = test_df_sample[di.tab_cols + di.text_cols].values\n",
    "    test_vals = test_df[di.tab_cols + di.text_cols].iloc[:10].values\n",
    "    # train_sample = train_df.sample(1000, random_state=55)\n",
    "    # train_sample_y = train_sample[di.label_col]\n",
    "    # train_sample_vals = train_sample[di.tab_cols + di.text_cols].values\n",
    "    # return model.predict(train_sample_vals), train_sample_y\n",
    "    return model.predict(test_vals), test_df[di.label_col].iloc[:10].values\n",
    "    return (\n",
    "        model.predict(test_sample_vals),\n",
    "        test_df_sample[di.label_col].values,\n",
    "        model.predict(test_vals),\n",
    "        test_df[di.label_col].values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_type, ds_type, test_set_size=100, tab_scale_factor=2):\n",
    "    di = get_dataset_info(ds_type, model_type)\n",
    "    # Data\n",
    "    train_df = load_dataset(\n",
    "        di.ds_name, split=\"train\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name, split=\"test\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df_sample = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if model_type == \"all_text\":\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            # model=di.text_model_name,\n",
    "            model=\"../models/wine/glowing-morning-9/checkpoint-6705\",\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert all columns to a single string\n",
    "\n",
    "        def cols_to_str_fn(array):\n",
    "            return \" | \".join(\n",
    "                [f\"{col}: {val}\" for col, val in zip(\n",
    "                    di.tab_cols + di.text_cols, array)]\n",
    "            )\n",
    "\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "            # cols=di.tab_cols + di.text_cols\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(di.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(di.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\n",
    "            \"category\")\n",
    "        test_df_sample[di.categorical_cols] = test_df_sample[\n",
    "            di.categorical_cols\n",
    "        ].astype(\"category\")\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42)\n",
    "        tab_model.fit(train_df[di.tab_cols], y_train)\n",
    "\n",
    "        if model_type == \"ensemble_50\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.5,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_75\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.75,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_25\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.25,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                di.ds_name, split=\"validation\", download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\n",
    "                \"category\")\n",
    "            y_val = val_df[di.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            # text_val_preds = np.array(\n",
    "            #     [format_text_pred(pred) for pred in text_val_preds]\n",
    "            # )\n",
    "            text_val_preds = np.array(\n",
    "                [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[di.tab_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(stack_val_df)\n",
    "            for i in range(text_val_preds.shape[1]):\n",
    "                stack_val_df[f\"text_pred_{i}\"] = text_val_preds[:, i]\n",
    "            for i in range(tab_val_preds.shape[1]):\n",
    "                stack_val_df[f\"tab_pred_{i}\"] = tab_val_preds[:, i]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(random_state=42)\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    test_sample_vals = test_df_sample[di.tab_cols + di.text_cols].values\n",
    "    test_vals = test_df[di.tab_cols + di.text_cols].values\n",
    "\n",
    "    return (\n",
    "        model.predict(test_sample_vals),\n",
    "        test_df_sample[di.label_col].values,\n",
    "        model.predict(test_vals),\n",
    "        test_df[di.label_col].values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "10it [00:00, 77101.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on wine\n",
      "Test accuaracy: 0.1\n",
      "Test accuaracy (sample): 0.13\n",
      "Test % by label: [0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "Test % by label (sample): [0.12, 0.06, 0.13, 0.03, 0.1, 0.09, 0.01, 0.04, 0.04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"wine\"\n",
    "\n",
    "for model_type in [\n",
    "    # \"all_text\",\n",
    "    \"ensemble_25\",\n",
    "    #    \"ensemble_50\",\n",
    "    #    \"ensemble_75\",\n",
    "    #    \"stack\"\n",
    "]:\n",
    "    # sample_preds, sample_y, preds, y = run_model(\n",
    "    #     model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    # )\n",
    "    preds, y = run_model(model_type, ds_type=ds_type, tab_scale_factor=1)\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    # print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    # print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(\n",
    "        f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 29,  5, ...,  5,  3, 26])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  9, 23, ..., 23,  2, 15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
