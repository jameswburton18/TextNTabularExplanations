{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from src.dataset_info import get_dataset_info\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.utils import token_segments, text_ft_index_ends\n",
    "\n",
    "# from src.models import Model\n",
    "import lightgbm as lgb\n",
    "from src.models import WeightedEnsemble, StackModel, AllAsTextModel\n",
    "from src.joint_masker import JointMasker\n",
    "import argparse\n",
    "import scipy as sp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(\n",
    "    model_type, ds_type, test_set_size=100, tab_scale_factor=2, reverse=False\n",
    "):\n",
    "    di = get_dataset_info(ds_type, model_type)\n",
    "    # Data\n",
    "    train_df = load_dataset(\n",
    "        di.ds_name, split=\"train\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name, split=\"test\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df_sample = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if model_type in [\n",
    "        \"all_text\",\n",
    "        \"all_as_text_tnt_reorder\",\n",
    "        \"all_as_text_base_reorder\",\n",
    "    ]:\n",
    "        model_name = di.text_model_name\n",
    "        # Define how to convert all columns to a single string\n",
    "        if model_type == \"all_text\":\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [\n",
    "                        f\"{col}: {val}\"\n",
    "                        for col, val in zip(di.tab_cols + di.text_cols, array)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        elif model_type == \"all_as_text_tnt_reorder\":\n",
    "            # Reorder based on the new index order in di\n",
    "            def cols_to_str_fn(array):\n",
    "                original = di.tab_cols + di.text_cols\n",
    "                new = di.tnt_reorder_cols if not reverse else di.tnt_reorder_cols[::-1]\n",
    "                new_array = np.array([array[original.index(x)] for x in new])\n",
    "                return \" | \".join([f\"{col}: {val}\" for col, val in zip(new, new_array)])\n",
    "\n",
    "            num = 2 if not reverse else 4\n",
    "            model_name = model_name[:-1] + num\n",
    "        else:\n",
    "            # Reorder based on the new index order in di\n",
    "            def cols_to_str_fn(array):\n",
    "                original = di.tab_cols + di.text_cols\n",
    "                new = di.tnt_reorder_cols if not reverse else di.tnt_reorder_cols[::-1]\n",
    "                new_array = np.array([array[original.index(x)] for x in new])\n",
    "                return \" | \".join([f\"{col}: {val}\" for col, val in zip(new, new_array)])\n",
    "\n",
    "            num = 1 if not reverse else 3\n",
    "            model_name = model_name[:-1] + num\n",
    "\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "            # cols=di.tab_cols + di.text_cols\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(di.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(di.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\"category\")\n",
    "        test_df_sample[di.categorical_cols] = test_df_sample[\n",
    "            di.categorical_cols\n",
    "        ].astype(\"category\")\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42)\n",
    "        tab_model.fit(train_df[di.tab_cols], y_train)\n",
    "\n",
    "        if model_type == \"ensemble_50\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.5,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_75\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.75,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_25\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.25,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                di.ds_name, split=\"validation\", download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\"category\")\n",
    "            y_val = val_df[di.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            # text_val_preds = np.array(\n",
    "            #     [format_text_pred(pred) for pred in text_val_preds]\n",
    "            # )\n",
    "            text_val_preds = np.array(\n",
    "                [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[di.tab_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(stack_val_df)\n",
    "            for i in range(text_val_preds.shape[1]):\n",
    "                stack_val_df[f\"text_pred_{i}\"] = text_val_preds[:, i]\n",
    "            for i in range(tab_val_preds.shape[1]):\n",
    "                stack_val_df[f\"tab_pred_{i}\"] = tab_val_preds[:, i]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(random_state=42)\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    test_sample_vals = test_df_sample[di.tab_cols + di.text_cols].values\n",
    "    test_vals = test_df[di.tab_cols + di.text_cols].values\n",
    "\n",
    "    return (\n",
    "        model.predict(test_sample_vals),\n",
    "        test_df_sample[di.label_col].values,\n",
    "        model.predict(test_vals),\n",
    "        test_df[di.label_col].values,\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Job Postings (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 340kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 66.0MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 42.1MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 26.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2968.37it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 274kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 61.5MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 40.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.7MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.42it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2599.78it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 103003.54it/s]\n",
      "3182it [00:00, 55860.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on fake\n",
      "Test accuaracy: 0.9387177875549969\n",
      "Test accuaracy (sample): 0.93\n",
      "Test AUC: 0.8744072444723762\n",
      "Test AUC (sample): 0.808421052631579\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 227kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 63.0MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 41.7MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2797.45it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 230kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 66.8MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 40.4MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 27.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.36it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3120.76it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 81936.00it/s]\n",
      "3182it [00:00, 56346.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on fake\n",
      "Test accuaracy: 0.9619736015084852\n",
      "Test accuaracy (sample): 0.96\n",
      "Test AUC: 0.9189305642842179\n",
      "Test AUC (sample): 0.9136842105263159\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 269kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 70.2MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 41.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.2MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2981.73it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 328kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 63.1MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 39.0MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2992.37it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 93310.43it/s]\n",
      "3182it [00:00, 56101.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on fake\n",
      "Test accuaracy: 0.9783155248271528\n",
      "Test accuaracy (sample): 0.99\n",
      "Test AUC: 0.9337994439048545\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 554kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 68.5MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 37.3MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.5MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.36it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2955.12it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 275kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 66.7MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 38.6MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.9MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2947.51it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 332kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 72.2MB/s]\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfake\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mensemble_25\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mensemble_50\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# \"all_as_text_base_reorder\",\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ]:\n\u001b[0;32m---> 12\u001b[0m     sample_preds, sample_y, preds, y \u001b[39m=\u001b[39m run_model(\n\u001b[1;32m     13\u001b[0m         model_type, ds_type\u001b[39m=\u001b[39;49mds_type, tab_scale_factor\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mds_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest accuaracy: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39margmax(preds,\u001b[39m \u001b[39maxis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m \u001b[39m\u001b[39m==\u001b[39m\u001b[39m \u001b[39my)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 124\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model_type, ds_type, test_set_size, tab_scale_factor, reverse)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39melif\u001b[39;00m model_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstack\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m    For the stack model, we make predictions on the validation set. These predictions\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m    are then used as features for the stack model (another LightGBM model) along with\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m    the other tabular features. In doing so the stack model learns, depending on the\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m    tabular features, when to trust the tabular model and when to trust the text model.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     val_df \u001b[39m=\u001b[39m load_dataset(\n\u001b[1;32m    125\u001b[0m         di\u001b[39m.\u001b[39;49mds_name, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m, download_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mforce_redownload\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    126\u001b[0m     )\u001b[39m.\u001b[39mto_pandas()\n\u001b[1;32m    127\u001b[0m     val_df[di\u001b[39m.\u001b[39mcategorical_cols] \u001b[39m=\u001b[39m val_df[di\u001b[39m.\u001b[39mcategorical_cols]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m     y_val \u001b[39m=\u001b[39m val_df[di\u001b[39m.\u001b[39mlabel_col]\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/load.py:1782\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1781\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1782\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1783\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1784\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1785\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   1786\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1787\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1788\u001b[0m )\n\u001b[1;32m   1790\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1792\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1793\u001b[0m )\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/builder.py:872\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 872\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    873\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    874\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    875\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    876\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    877\u001b[0m     )\n\u001b[1;32m    878\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/builder.py:945\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m    944\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 945\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msplit_generators_kwargs)\n\u001b[1;32m    947\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:34\u001b[0m, in \u001b[0;36mParquet._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdata_files:\n\u001b[1;32m     33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdata_files\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m data_files \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdata_files)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_files, (\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m     36\u001b[0m     files \u001b[39m=\u001b[39m data_files\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/download/download_manager.py:564\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_and_extract\u001b[39m(\u001b[39mself\u001b[39m, url_or_urls):\n\u001b[1;32m    549\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[39m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload(url_or_urls))\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/download/download_manager.py:427\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    424\u001b[0m download_func \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[1;32m    426\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m--> 427\u001b[0m downloaded_path_or_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[1;32m    428\u001b[0m     download_func,\n\u001b[1;32m    429\u001b[0m     url_or_urls,\n\u001b[1;32m    430\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    431\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m    432\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[1;32m    433\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    434\u001b[0m )\n\u001b[1;32m    435\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    436\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading took \u001b[39m\u001b[39m{\u001b[39;00mduration\u001b[39m.\u001b[39mtotal_seconds()\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39m60\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/py_utils.py:443\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    441\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n\u001b[0;32m--> 443\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[1;32m    444\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    445\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n\u001b[1;32m    446\u001b[0m     ]\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     num_proc \u001b[39m=\u001b[39m num_proc \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(iterable) \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(iterable)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/py_utils.py:444\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    441\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n\u001b[1;32m    443\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 444\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m    445\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n\u001b[1;32m    446\u001b[0m     ]\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     num_proc \u001b[39m=\u001b[39m num_proc \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(iterable) \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(iterable)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/py_utils.py:363\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: _single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m pbar}\n\u001b[1;32m    362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m     mapped \u001b[39m=\u001b[39m [_single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m pbar]\n\u001b[1;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    365\u001b[0m         \u001b[39mreturn\u001b[39;00m mapped\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/py_utils.py:363\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: _single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m pbar}\n\u001b[1;32m    362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m     mapped \u001b[39m=\u001b[39m [_single_map_nested((function, v, types, \u001b[39mNone\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m pbar]\n\u001b[1;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    365\u001b[0m         \u001b[39mreturn\u001b[39;00m mapped\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/py_utils.py:346\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m# Singleton first to spare some computation\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    348\u001b[0m \u001b[39m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m<\u001b[39m logging\u001b[39m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/download/download_manager.py:453\u001b[0m, in \u001b[0;36mDownloadManager._download\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    451\u001b[0m     \u001b[39m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     url_or_filename \u001b[39m=\u001b[39m url_or_path_join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(url_or_filename, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/file_utils.py:182\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    181\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    183\u001b[0m         url_or_filename,\n\u001b[1;32m    184\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    185\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    186\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    187\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    188\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    189\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    190\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    191\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    192\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[1;32m    193\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[1;32m    194\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    197\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/file_utils.py:575\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\u001b[0m\n\u001b[1;32m    573\u001b[0m         ftp_get(url, temp_file)\n\u001b[1;32m    574\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m         http_get(\n\u001b[1;32m    576\u001b[0m             url,\n\u001b[1;32m    577\u001b[0m             temp_file,\n\u001b[1;32m    578\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    579\u001b[0m             resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    580\u001b[0m             headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    581\u001b[0m             cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    582\u001b[0m             max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    583\u001b[0m             desc\u001b[39m=\u001b[39;49mdownload_desc,\n\u001b[1;32m    584\u001b[0m         )\n\u001b[1;32m    586\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    587\u001b[0m shutil\u001b[39m.\u001b[39mmove(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/file_utils.py:357\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m resume_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    356\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mRange\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbytes=\u001b[39m\u001b[39m{\u001b[39;00mresume_size\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 357\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    358\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    359\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    360\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    361\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    362\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    363\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    364\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    365\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m416\u001b[39m:  \u001b[39m# Range not satisfiable\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/file_utils.py:318\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    316\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    319\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds_type = \"fake\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    # \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )\n",
    "\n",
    "for model_type in [\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    # \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1, reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} reverse on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB  (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 264kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.5MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.96MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.07MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2163.87it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 332kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 39.3MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.18MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.50MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2989.53it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 696728.24it/s]\n",
      "200it [00:00, 820803.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on imdb_genre\n",
      "Test accuaracy: 0.78\n",
      "Test accuaracy (sample): 0.79\n",
      "Test AUC: 0.8530677609848865\n",
      "Test AUC (sample): 0.8262626262626263\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 212kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.4MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.88MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 5.23MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2361.66it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 292kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.4MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.85MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.82MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3021.11it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 586615.94it/s]\n",
      "200it [00:00, 726286.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on imdb_genre\n",
      "Test accuaracy: 0.785\n",
      "Test accuaracy (sample): 0.79\n",
      "Test AUC: 0.8646782103893504\n",
      "Test AUC (sample): 0.8513131313131314\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 286kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 39.0MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.22MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.60MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2757.60it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 1.20MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 35.2MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 8.00MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.31MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.68it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2991.66it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 580928.53it/s]\n",
      "200it [00:00, 665234.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on imdb_genre\n",
      "Test accuaracy: 0.775\n",
      "Test accuaracy (sample): 0.75\n",
      "Test AUC: 0.8520668601741568\n",
      "Test AUC (sample): 0.8387878787878787\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 274kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.4MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.80MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.45MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2964.87it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 614kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 5.88MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.79MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.14MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2944.06it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 356kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 46.2MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.81MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.17MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2873.47it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 566032.93it/s]\n",
      "200it [00:00, 721290.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on imdb_genre\n",
      "Test accuaracy: 0.74\n",
      "Test accuaracy (sample): 0.73\n",
      "Test AUC: 0.8106295666099489\n",
      "Test AUC (sample): 0.7846464646464647\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 906/906 [00:00<00:00, 249kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 225.28 KiB, generated: 320.45 KiB, post-processed: Unknown size, total: 545.73 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.4k/34.4k [00:00<00:00, 33.8MB/s]\n",
      "Downloading data: 100%|██████████| 145k/145k [00:00<00:00, 7.02MB/s]]\n",
      "Downloading data: 100%|██████████| 51.0k/51.0k [00:00<00:00, 5.42MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2258.24it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 906/906 [00:00<00:00, 321kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 225.28 KiB, generated: 320.45 KiB, post-processed: Unknown size, total: 545.73 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.4k/34.4k [00:00<00:00, 39.3MB/s]\n",
      "Downloading data: 100%|██████████| 145k/145k [00:00<00:00, 6.98MB/s]]\n",
      "Downloading data: 100%|██████████| 51.0k/51.0k [00:00<00:00, 5.43MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.52it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2688.66it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_text on imdb_genre\n",
      "Test accuaracy: 0.72\n",
      "Test accuaracy (sample): 0.69\n",
      "Test AUC: 0.8146331698528675\n",
      "Test AUC (sample): 0.8004040404040405\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 416kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 35.5MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.83MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.23MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.57it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2807.43it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 658kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 33.5MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.80MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.20MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2807.43it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_as_text_tnt_reorder on imdb_genre\n",
      "Test accuaracy: 0.625\n",
      "Test accuaracy (sample): 0.64\n",
      "Test AUC: 0.8062256030427385\n",
      "Test AUC (sample): 0.8028282828282829\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 315kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.2MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.89MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.60MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3003.08it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 1.04MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.9MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.22MB/s]]\n",
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.50MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3036.42it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_as_text_base_reorder on imdb_genre\n",
      "Test accuaracy: 0.645\n",
      "Test accuaracy (sample): 0.66\n",
      "Test AUC: 0.8113301971774597\n",
      "Test AUC (sample): 0.8084848484848485\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"imdb_genre\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )\n",
    "\n",
    "for model_type in [\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1, reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} reverse on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kickstarter  (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 296kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 38.7MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 72.1MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 44.5MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2803.68it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 302kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 38.6MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 75.7MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 47.7MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2799.94it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 333675.74it/s]\n",
      "21626it [00:00, 309146.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on kick\n",
      "Test accuaracy: 0.6999445112364746\n",
      "Test accuaracy (sample): 0.66\n",
      "Test AUC: 0.7410626481173135\n",
      "Test AUC (sample): 0.7507507507507508\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 211kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 36.9MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 72.3MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1996.65it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 259kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 39.7MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 75.6MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 44.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2784.45it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 378205.95it/s]\n",
      "21626it [00:00, 319307.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on kick\n",
      "Test accuaracy: 0.7136779802090076\n",
      "Test accuaracy (sample): 0.67\n",
      "Test AUC: 0.7735201424600657\n",
      "Test AUC (sample): 0.7691977691977692\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 293kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 38.1MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 76.1MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 45.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2875.44it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 667kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 38.9MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 75.1MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 44.5MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2689.23it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 424095.45it/s]\n",
      "21626it [00:00, 305686.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on kick\n",
      "Test accuaracy: 0.7177471562008694\n",
      "Test accuaracy (sample): 0.69\n",
      "Test AUC: 0.7679046143380706\n",
      "Test AUC (sample): 0.7301587301587301\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 676kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 39.5MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 76.5MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 46.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2641.25it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 878kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 38.9MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 78.3MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.69it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2769.13it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 392kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 38.9MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 76.5MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2653.50it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 439654.51it/s]\n",
      "21626it [00:00, 328720.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on kick\n",
      "Test accuaracy: 0.7072967724035882\n",
      "Test accuaracy (sample): 0.65\n",
      "Test AUC: 0.7491010899483486\n",
      "Test AUC (sample): 0.7211497211497212\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 840/840 [00:00<00:00, 340kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 18.14 MiB, generated: 30.71 MiB, post-processed: Unknown size, total: 48.85 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.29M/2.29M [00:00<00:00, 38.2MB/s]\n",
      "Downloading data: 100%|██████████| 12.9M/12.9M [00:00<00:00, 73.9MB/s]\n",
      "Downloading data: 100%|██████████| 3.81M/3.81M [00:00<00:00, 48.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2654.06it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 840/840 [00:00<00:00, 278kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 18.14 MiB, generated: 30.71 MiB, post-processed: Unknown size, total: 48.85 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.29M/2.29M [00:00<00:00, 38.0MB/s]\n",
      "Downloading data: 100%|██████████| 12.9M/12.9M [00:00<00:00, 76.8MB/s]\n",
      "Downloading data: 100%|██████████| 3.81M/3.81M [00:00<00:00, 43.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2588.54it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_text on kick\n",
      "Test accuaracy: 0.7351798760750948\n",
      "Test accuaracy (sample): 0.66\n",
      "Test AUC: 0.7875153407217366\n",
      "Test AUC (sample): 0.7696267696267696\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 320kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 39.4MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 77.1MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 45.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2641.80it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 348kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 37.5MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 78.9MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 44.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2737.80it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_as_text_tnt_reorder on kick\n",
      "Test accuaracy: 0.711088504577823\n",
      "Test accuaracy (sample): 0.69\n",
      "Test AUC: 0.7393402976394993\n",
      "Test AUC (sample): 0.676962676962677\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 335kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 39.6MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 73.6MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 44.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.40it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2941.99it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 884kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 38.7MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 76.2MB/s]\n",
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2829.53it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_as_text_base_reorder on kick\n",
      "Test accuaracy: 0.7124294830296865\n",
      "Test accuaracy (sample): 0.69\n",
      "Test AUC: 0.7394847670821818\n",
      "Test AUC (sample): 0.670956670956671\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"kick\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "for model_type in [\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1, reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} reverse on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jigsaw (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 418kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 37.8MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 84.7MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 57.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.23it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3032.76it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 602kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 49.6MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 91.6MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 63.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.52it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3066.01it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 444783.03it/s]\n",
      "25000it [00:00, 410541.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on jigsaw\n",
      "Test accuaracy: 0.94256\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9321638495642937\n",
      "Test AUC (sample): 0.8378947368421054\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 645kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 41.8MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 91.7MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 65.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3002.37it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 624kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 40.6MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 87.9MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 64.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.41it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2986.69it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 420692.48it/s]\n",
      "25000it [00:00, 444769.83it/s]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"jigsaw\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "for model_type in [\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1, reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} reverse on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 489kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 6.48MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 1.27MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 595kB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2763.05it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 211kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.41MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 11.6MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 4.83MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2990.24it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 921825.05it/s]\n",
      "1273it [00:00, 1303233.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on prod_sent\n",
      "Test accuaracy: 0.8908091123330715\n",
      "Test accuaracy (sample): 0.95\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 666kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.56MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.70MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.73MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2791.24it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 677kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.58MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.66MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.53MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.56it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2907.33it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 731990.23it/s]\n",
      "1273it [00:00, 1375411.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on prod_sent\n",
      "Test accuaracy: 0.8695993715632364\n",
      "Test accuaracy (sample): 0.9\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 761kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.63MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.73MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.67MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2988.82it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 704kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.59MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 10.1MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.72MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2956.51it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 530924.56it/s]\n",
      "1273it [00:00, 1430310.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on prod_sent\n",
      "Test accuaracy: 0.6614296936370778\n",
      "Test accuaracy (sample): 0.64\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 736kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 430.99 KiB, generated: 758.28 KiB, post-processed: Unknown size, total: 1.16 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.1k/54.1k [00:00<00:00, 8.07MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 10.0MB/s]]\n",
      "Downloading data: 100%|██████████| 90.4k/90.4k [00:00<00:00, 5.61MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2615.99it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 197kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 430.99 KiB, generated: 758.28 KiB, post-processed: Unknown size, total: 1.16 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.1k/54.1k [00:00<00:00, 5.69MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.85MB/s]]\n",
      "Downloading data: 100%|██████████| 90.4k/90.4k [00:00<00:00, 5.66MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2977.50it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:02<00:00, 108MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_text on prod_sent\n",
      "Test accuaracy: 0.8476040848389631\n",
      "Test accuaracy (sample): 0.88\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 448kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 7.92MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.80MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.66MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2966.97it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 632kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.55MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.79MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 6.21MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2580.58it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 216kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.76MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.82MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.73MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2945.44it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 907858.01it/s]\n",
      "1273it [00:00, 1387204.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on prod_sent\n",
      "Test accuaracy: 0.6496465043205027\n",
      "Test accuaracy (sample): 0.68\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"prod_sent\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    # \"all_as_text_tnt_reorder\",\n",
    "    # \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    # print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    # print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset wine, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 673/673 [00:00<00:00, 721kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 15.47 MiB, generated: 29.51 MiB, post-processed: Unknown size, total: 44.99 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.95M/1.95M [00:00<00:00, 4.27MB/s]\n",
      "Downloading data: 100%|██████████| 11.0M/11.0M [00:00<00:00, 15.9MB/s]\n",
      "Downloading data: 100%|██████████| 3.24M/3.24M [00:00<00:00, 6.04MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:04<00:00,  1.67s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1963.63it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 673/673 [00:00<00:00, 410kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 15.47 MiB, generated: 29.51 MiB, post-processed: Unknown size, total: 44.99 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.95M/1.95M [00:00<00:00, 36.0MB/s]\n",
      "Downloading data: 100%|██████████| 11.0M/11.0M [00:00<00:00, 72.1MB/s]\n",
      "Downloading data: 100%|██████████| 3.24M/3.24M [00:00<00:00, 43.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2816.23it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 645277.54it/s]\n",
      "21031it [00:00, 470854.79it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwine\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m      4\u001b[0m     \u001b[39m# \"all_text\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mensemble_25\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39m#     model_type, ds_type=ds_type, tab_scale_factor=1\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     preds, y \u001b[39m=\u001b[39m run_model(model_type, ds_type\u001b[39m=\u001b[39mds_type, tab_scale_factor\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mds_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest accuaracy: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39margmax(preds,\u001b[39m \u001b[39maxis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m \u001b[39m\u001b[39m==\u001b[39m\u001b[39m \u001b[39my)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "ds_type = \"wine\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    # \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    # sample_preds, sample_y, preds, y = run_model(\n",
    "    #     model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    # )\n",
    "    preds, y = run_model(model_type, ds_type=ds_type, tab_scale_factor=1)\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    # print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    # print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "for model_type in [\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    # \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1, reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} reverse on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 29,  5, ...,  5,  3, 26])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  9, 23, ..., 23,  2, 15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
