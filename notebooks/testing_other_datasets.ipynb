{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product_sentiment_machine_hack', 'jigsaw_unintended_bias', 'jigsaw_unintended_bias100K', 'google_qa_label', 'google_qa_answer_helpful', 'google_qa_answer_plausible', 'google_qa_answer_type_procedure', 'google_qa_answer_type_reason_explanation', 'google_qa_question_type_reason_explanation', 'google_qa_answer_satisfaction', 'women_clothing_review', 'melbourne_airbnb', 'mercari_price_suggestion', 'ae_price_prediction', 'mercari_price_suggestion100K', 'imdb_genre_prediction', 'fake_job_postings', 'kick_starter_funding', 'jc_penney_products', 'wine_reviews', 'news_popularity', 'news_channel', 'news_popularity2', 'fake_job_postings2', 'bookprice_prediction', 'data_scientist_salary', 'california_house_price']\n",
      "Downloading /home/james/.auto_mm_bench/datasets/machine_hack_sentiment_analysis/train.csv from https://automl-mm-bench.s3.amazonaws.com/machine_hack_product_sentiment/train.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611k/611k [00:00<00:00, 1.62MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/james/.auto_mm_bench/datasets/machine_hack_sentiment_analysis/test.csv from https://automl-mm-bench.s3.amazonaws.com/machine_hack_product_sentiment/dev.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154k/154k [00:00<00:00, 571kiB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Text_ID                                Product_Description  \\\n",
      "0           5743     2333  #techcrunch #google This Post Has Nothing to d...   \n",
      "1           3042     3448  Data is the new oil. (Companies like Google an...   \n",
      "2           4359      720  my sister is throwing the Google sxsw party to...   \n",
      "3           5685     2328  Clear +succinct visions make for great UX (thi...   \n",
      "4           2078     4955  40% of Google Maps use is mobile marissamayer ...   \n",
      "...          ...      ...                                                ...   \n",
      "5086        1096     3235  I'm eyeing the grilled cheese stand for after ...   \n",
      "5087        3519     2728  Let's make that a temp-to-hire position RT @me...   \n",
      "5088         212     1370  @mention -&gt; RT @mention New #UberSocial for...   \n",
      "5089        2786     2530  @mention Did you find out the hours of the app...   \n",
      "5090        3942     8162  &quot;At SXSW, Apple schools the marketing exp...   \n",
      "\n",
      "      Product_Type  Sentiment  \n",
      "0                9          2  \n",
      "1                3          1  \n",
      "2                3          3  \n",
      "3                9          2  \n",
      "4                9          3  \n",
      "...            ...        ...  \n",
      "5086             9          2  \n",
      "5087             9          2  \n",
      "5088             9          3  \n",
      "5089             9          2  \n",
      "5090             2          3  \n",
      "\n",
      "[5091 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from auto_mm_bench.datasets import dataset_registry\n",
    "\n",
    "# use these keys to specify which dataset to load\n",
    "print(dataset_registry.list_keys())\n",
    "\n",
    "train_dataset = dataset_registry.create(\"product_sentiment_machine_hack\", \"train\")\n",
    "test_dataset = dataset_registry.create(\"product_sentiment_machine_hack\", \"test\")\n",
    "print(train_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_columns ['Product_Description', 'Product_Type']\n",
      "feature_types ['text', 'categorical']\n",
      "label_columns ['Sentiment']\n",
      "label_types ['categorical']\n",
      "metric acc\n",
      "problem_type multiclass\n"
     ]
    }
   ],
   "source": [
    "info_cols = [\n",
    "    # 'splits',\n",
    "    \"feature_columns\",\n",
    "    \"feature_types\",\n",
    "    \"label_columns\",\n",
    "    \"label_types\",\n",
    "    #  'data',\n",
    "    \"metric\",\n",
    "    \"problem_type\",\n",
    "]\n",
    "\n",
    "for col in info_cols:\n",
    "    print(col, getattr(train_dataset, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.label_columns[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.                                         \n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 73.36ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "Pushing split validation to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 27.16ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "Pushing split test to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 2062.10ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset_name = \"product_sentiment_machine_hack\"\n",
    "cols = train_dataset.feature_columns + train_dataset.label_columns\n",
    "\n",
    "train_txt = train_dataset.data[cols]\n",
    "test_txt = test_dataset.data[cols]\n",
    "train_dataset.data[train_dataset.label_columns[0]] = train_dataset.data[\n",
    "    train_dataset.label_columns[0]\n",
    "].astype(int)\n",
    "test_dataset.data[train_dataset.label_columns[0]] = test_dataset.data[\n",
    "    train_dataset.label_columns[0]\n",
    "].astype(int)\n",
    "\n",
    "# load dataset from dataframe\n",
    "train_ds = Dataset.from_pandas(train_txt)\n",
    "train_ds = train_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "test_ds = Dataset.from_pandas(test_txt)\n",
    "test_ds = test_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "\n",
    "\n",
    "train_ds = train_ds.train_test_split(\n",
    "    test_size=0.15, seed=42, stratify_by_column=train_dataset.label_columns[0]\n",
    ")\n",
    "ds = DatasetDict(\n",
    "    {\"train\": train_ds[\"train\"],\n",
    "        \"validation\": train_ds[\"test\"], \"test\": test_ds}\n",
    ")\n",
    "ds.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Product_Description', 'Product_Type', 'Sentiment'],\n",
       "        num_rows: 4327\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Product_Description', 'Product_Type', 'Sentiment'],\n",
       "        num_rows: 764\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Product_Description', 'Product_Type', 'Sentiment'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    748\n",
       "3    436\n",
       "1     75\n",
       "0     14\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.data[test_dataset.label_columns[0]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "502 Server Error: Bad Gateway for url: https://huggingface.co/api/datasets/james-burton/product_sentiment_machine_hack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m my_ds \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mjames-burton/product_sentiment_machine_hack\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/load.py:1759\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1755\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1756\u001b[0m )\n\u001b[1;32m   1758\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1759\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1760\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1761\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1762\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1763\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1764\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1765\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1766\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1767\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1768\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1769\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1770\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1771\u001b[0m )\n\u001b[1;32m   1773\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/load.py:1496\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1495\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1496\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1497\u001b[0m     path,\n\u001b[1;32m   1498\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1499\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1500\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1501\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1502\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1503\u001b[0m )\n\u001b[1;32m   1505\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/load.py:1218\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1214\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1216\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hugging Face Hub either: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e1)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                 ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m-> 1218\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1222\u001b[0m     )\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/load.py:1202\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1188\u001b[0m             path,\n\u001b[1;32m   1189\u001b[0m             revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1192\u001b[0m             dynamic_modules_path\u001b[39m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1193\u001b[0m         )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m   1194\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1195\u001b[0m         \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1196\u001b[0m             path,\n\u001b[1;32m   1197\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1198\u001b[0m             data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1199\u001b[0m             data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1200\u001b[0m             download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1201\u001b[0m             download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m-> 1202\u001b[0m         )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1203\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m   1204\u001b[0m     \u001b[39mException\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m ) \u001b[39mas\u001b[39;00m e1:  \u001b[39m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/load.py:757\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_module\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetModule:\n\u001b[0;32m--> 757\u001b[0m     hfh_dataset_info \u001b[39m=\u001b[39m hf_api_dataset_info(\n\u001b[1;32m    758\u001b[0m         HfApi(config\u001b[39m.\u001b[39;49mHF_ENDPOINT),\n\u001b[1;32m    759\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    760\u001b[0m         revision\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrevision,\n\u001b[1;32m    761\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[1;32m    762\u001b[0m         timeout\u001b[39m=\u001b[39;49m\u001b[39m100.0\u001b[39;49m,\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    764\u001b[0m     patterns \u001b[39m=\u001b[39m (\n\u001b[1;32m    765\u001b[0m         sanitize_patterns(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files)\n\u001b[1;32m    766\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    767\u001b[0m         \u001b[39melse\u001b[39;00m get_data_patterns_in_dataset_repository(hfh_dataset_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir)\n\u001b[1;32m    768\u001b[0m     )\n\u001b[1;32m    769\u001b[0m     data_files \u001b[39m=\u001b[39m DataFilesDict\u001b[39m.\u001b[39mfrom_hf_repo(\n\u001b[1;32m    770\u001b[0m         patterns,\n\u001b[1;32m    771\u001b[0m         dataset_info\u001b[39m=\u001b[39mhfh_dataset_info,\n\u001b[1;32m    772\u001b[0m         base_path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir,\n\u001b[1;32m    773\u001b[0m         allowed_extensions\u001b[39m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    774\u001b[0m     )\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/datasets/utils/_hf_hub_fixes.py:152\u001b[0m, in \u001b[0;36mdataset_info\u001b[0;34m(hf_api, repo_id, revision, timeout, use_auth_token)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m hf_api\u001b[39m.\u001b[39mdataset_info(\n\u001b[1;32m    146\u001b[0m         repo_id,\n\u001b[1;32m    147\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m    148\u001b[0m         token\u001b[39m=\u001b[39mtoken,\n\u001b[1;32m    149\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# the `token` parameter is deprecated in huggingface_hub>=0.10.0\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mreturn\u001b[39;00m hf_api\u001b[39m.\u001b[39;49mdataset_info(repo_id, revision\u001b[39m=\u001b[39;49mrevision, timeout\u001b[39m=\u001b[39;49mtimeout, use_auth_token\u001b[39m=\u001b[39;49muse_auth_token)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1691\u001b[0m, in \u001b[0;36mHfApi.dataset_info\u001b[0;34m(self, repo_id, revision, timeout, files_metadata, token)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mblobs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(path, headers\u001b[39m=\u001b[39mheaders, timeout\u001b[39m=\u001b[39mtimeout, params\u001b[39m=\u001b[39mparams)\n\u001b[0;32m-> 1691\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1692\u001b[0m d \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mjson()\n\u001b[1;32m   1693\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetInfo(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39md)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:318\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 502 Server Error: Bad Gateway for url: https://huggingface.co/api/datasets/james-burton/product_sentiment_machine_hack"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "my_ds = load_dataset(\"james-burton/product_sentiment_machine_hack\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
