{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from src.dataset_info import get_dataset_info\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.utils import token_segments, text_ft_index_ends\n",
    "\n",
    "# from src.models import Model\n",
    "import lightgbm as lgb\n",
    "from src.models import WeightedEnsemble, StackModel, AllAsTextModel\n",
    "from src.joint_masker import JointMasker\n",
    "import argparse\n",
    "import scipy as sp\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_type, ds_type, test_set_size=100, tab_scale_factor=2):\n",
    "    di = get_dataset_info(ds_type, model_type)\n",
    "    # Data\n",
    "    train_df = load_dataset(\n",
    "        di.ds_name, split=\"train\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name, split=\"test\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df_sample = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if model_type == \"all_text\":\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            # model=di.text_model_name,\n",
    "            model=\"../models/wine/glowing-morning-9/checkpoint-6705\",\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert all columns to a single string\n",
    "\n",
    "        def cols_to_str_fn(array):\n",
    "            return \" | \".join(\n",
    "                [f\"{col}: {val}\" for col, val in zip(di.tab_cols + di.text_cols, array)]\n",
    "            )\n",
    "\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "            # cols=di.tab_cols + di.text_cols\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(di.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(di.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\"category\")\n",
    "        test_df_sample[di.categorical_cols] = test_df_sample[\n",
    "            di.categorical_cols\n",
    "        ].astype(\"category\")\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42)\n",
    "        tab_model.fit(train_df[di.tab_cols], y_train)\n",
    "\n",
    "        if model_type == \"ensemble_50\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.5,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_75\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.75,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_25\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.25,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                di.ds_name, split=\"validation\", download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\"category\")\n",
    "            y_val = val_df[di.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            # text_val_preds = np.array(\n",
    "            #     [format_text_pred(pred) for pred in text_val_preds]\n",
    "            # )\n",
    "            text_val_preds = np.array(\n",
    "                [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[di.tab_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(stack_val_df)\n",
    "            for i in range(text_val_preds.shape[1]):\n",
    "                stack_val_df[f\"text_pred_{i}\"] = text_val_preds[:, i]\n",
    "            for i in range(tab_val_preds.shape[1]):\n",
    "                stack_val_df[f\"tab_pred_{i}\"] = tab_val_preds[:, i]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(random_state=42)\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    test_sample_vals = test_df_sample[di.tab_cols + di.text_cols].values\n",
    "    test_vals = test_df[di.tab_cols + di.text_cols].values\n",
    "\n",
    "    return (\n",
    "        model.predict(test_sample_vals),\n",
    "        test_df_sample[di.label_col].values,\n",
    "        model.predict(test_vals),\n",
    "        test_df[di.label_col].values,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Job Postings (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 1.07MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 40.6MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 63.5MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.41it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2944.06it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 751kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 40.7MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 65.3MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.7MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.40it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2437.60it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 73752.49it/s]\n",
      "3182it [00:00, 54308.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on fake\n",
      "Test accuaracy: 0.9387177875549969\n",
      "Test accuaracy (sample): 0.93\n",
      "Test AUC: 0.8744072444723762\n",
      "Test AUC (sample): 0.808421052631579\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 174kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 39.9MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 69.6MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 24.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.42it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2444.23it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 275kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 42.4MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 69.6MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 26.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2691.53it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 97769.32it/s]\n",
      "3182it [00:00, 53804.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on fake\n",
      "Test accuaracy: 0.9619736015084852\n",
      "Test accuaracy (sample): 0.96\n",
      "Test AUC: 0.9189305642842179\n",
      "Test AUC (sample): 0.9136842105263159\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 295kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 40.1MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 67.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 27.2MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2946.13it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 276kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 42.3MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 62.5MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 27.2MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2782.60it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 79845.88it/s]\n",
      "3182it [00:00, 55415.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on fake\n",
      "Test accuaracy: 0.9783155248271528\n",
      "Test accuaracy (sample): 0.99\n",
      "Test AUC: 0.9337994439048545\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset fake, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 709/709 [00:00<00:00, 535kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.55 MiB, generated: 20.58 MiB, post-processed: Unknown size, total: 32.13 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_all_text-b16faba3acce3185/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 4.97MB/s]\n",
      "Downloading data: 100%|██████████| 8.24M/8.24M [00:00<00:00, 13.0MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:01<00:00, 801kB/s] \n",
      "Downloading data files: 100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2188.71it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_all_text-b16faba3acce3185/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 709/709 [00:00<00:00, 306kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.55 MiB, generated: 20.58 MiB, post-processed: Unknown size, total: 32.13 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_all_text-b16faba3acce3185/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 35.7MB/s]\n",
      "Downloading data: 100%|██████████| 8.24M/8.24M [00:00<00:00, 66.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 27.7MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2583.76it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_all_text-b16faba3acce3185/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_text on fake\n",
      "Test accuaracy: 0.12507856693903205\n",
      "Test accuaracy (sample): 0.1\n",
      "Test AUC: 0.5576329772039079\n",
      "Test AUC (sample): 0.5157894736842106\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 249kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 37.6MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 67.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 26.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:33<00:00, 11.14s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2765.48it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 235kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 39.7MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 65.8MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 27.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2757.60it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 325kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 36.9MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 65.7MB/s]\n",
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 28.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2585.35it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 81127.74it/s]\n",
      "3182it [00:00, 54071.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on fake\n",
      "Test accuaracy: 0.943117536140792\n",
      "Test accuaracy (sample): 0.93\n",
      "Test AUC: 0.9087501666380999\n",
      "Test AUC (sample): 0.9557894736842105\n",
      "Test % by label: [0.9566310496543055, 0.04336895034569453]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"fake\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(\n",
    "        f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB  (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 501kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 6.86MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.30MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 35.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2741.38it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 324kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.21MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.76MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 40.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3126.19it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 785450.19it/s]\n",
      "200it [00:00, 502011.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on imdb_genre\n",
      "Test accuaracy: 0.78\n",
      "Test accuaracy (sample): 0.79\n",
      "Test AUC: 0.8530677609848865\n",
      "Test AUC (sample): 0.8262626262626263\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 337kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 5.32MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.29MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 42.2MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3185.55it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 374kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.88MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.06MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 40.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2476.46it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 788403.01it/s]\n",
      "200it [00:00, 976555.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on imdb_genre\n",
      "Test accuaracy: 0.785\n",
      "Test accuaracy (sample): 0.79\n",
      "Test AUC: 0.8646782103893504\n",
      "Test AUC (sample): 0.8513131313131314\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 279kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.25MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.16MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3136.32it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 1.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.87MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.18MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 39.0MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2790.00it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 733269.93it/s]\n",
      "200it [00:00, 995089.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on imdb_genre\n",
      "Test accuaracy: 0.775\n",
      "Test accuaracy (sample): 0.75\n",
      "Test AUC: 0.8520668601741568\n",
      "Test AUC (sample): 0.8387878787878787\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 906/906 [00:00<00:00, 297kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 225.28 KiB, generated: 320.45 KiB, post-processed: Unknown size, total: 545.73 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.0k/51.0k [00:00<00:00, 7.45MB/s]\n",
      "Downloading data: 100%|██████████| 145k/145k [00:00<00:00, 7.04MB/s]]\n",
      "Downloading data: 100%|██████████| 34.4k/34.4k [00:00<00:00, 40.0MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3201.76it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 906/906 [00:00<00:00, 387kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 225.28 KiB, generated: 320.45 KiB, post-processed: Unknown size, total: 545.73 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.0k/51.0k [00:00<00:00, 7.01MB/s]\n",
      "Downloading data: 100%|██████████| 145k/145k [00:00<00:00, 7.75MB/s]]\n",
      "Downloading data: 100%|██████████| 34.4k/34.4k [00:00<00:00, 39.5MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2916.76it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_text on imdb_genre\n",
      "Test accuaracy: 0.03\n",
      "Test accuaracy (sample): 0.01\n",
      "Test AUC: 0.5008507656891202\n",
      "Test AUC (sample): 0.46626262626262627\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 323kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.01MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.91MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 40.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2901.96it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 325kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 6.89MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.82MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 36.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.55it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3337.64it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 436kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 5.45MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.17MB/s]]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.0MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2961.38it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 709696.11it/s]\n",
      "200it [00:00, 838022.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on imdb_genre\n",
      "Test accuaracy: 0.74\n",
      "Test accuaracy (sample): 0.73\n",
      "Test AUC: 0.8106295666099489\n",
      "Test AUC (sample): 0.7846464646464647\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"imdb_genre\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(\n",
    "        f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kickstarter  (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 341kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.9MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 74.6MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 37.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.40it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2825.08it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 253kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 46.9MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 76.1MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 36.2MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.42it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2407.29it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 379575.02it/s]\n",
      "21626it [00:00, 334115.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on kick\n",
      "Test accuaracy: 0.6999445112364746\n",
      "Test accuaracy (sample): 0.66\n",
      "Test AUC: 0.7410626481173135\n",
      "Test AUC (sample): 0.7507507507507508\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 389kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 44.5MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 77.6MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 37.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2418.86it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 280kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 42.6MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 74.7MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 36.5MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2697.30it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 384798.53it/s]\n",
      "21626it [00:00, 340815.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on kick\n",
      "Test accuaracy: 0.7136779802090076\n",
      "Test accuaracy (sample): 0.67\n",
      "Test AUC: 0.7735201424600657\n",
      "Test AUC (sample): 0.7691977691977692\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 310kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 42.3MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 75.8MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 35.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2402.69it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 377kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.2MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 79.6MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 36.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2652.94it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 442904.33it/s]\n",
      "21626it [00:00, 346552.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on kick\n",
      "Test accuaracy: 0.7177471562008694\n",
      "Test accuaracy (sample): 0.69\n",
      "Test AUC: 0.7679046143380706\n",
      "Test AUC (sample): 0.7301587301587301\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 840/840 [00:00<00:00, 376kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 18.14 MiB, generated: 30.71 MiB, post-processed: Unknown size, total: 48.85 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.81M/3.81M [00:00<00:00, 44.8MB/s]\n",
      "Downloading data: 100%|██████████| 12.9M/12.9M [00:00<00:00, 77.5MB/s]\n",
      "Downloading data: 100%|██████████| 2.29M/2.29M [00:00<00:00, 37.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2503.56it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 840/840 [00:00<00:00, 270kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 18.14 MiB, generated: 30.71 MiB, post-processed: Unknown size, total: 48.85 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.81M/3.81M [00:00<00:00, 45.3MB/s]\n",
      "Downloading data: 100%|██████████| 12.9M/12.9M [00:00<00:00, 79.2MB/s]\n",
      "Downloading data: 100%|██████████| 2.29M/2.29M [00:00<00:00, 38.0MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2643.47it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_all_text-359fb6748cb2726a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_text on kick\n",
      "Test accuaracy: 0.0676500508646999\n",
      "Test accuaracy (sample): 0.05\n",
      "Test AUC: 0.5144450465889125\n",
      "Test AUC (sample): 0.5066495066495067\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n",
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 1.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.7MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 73.5MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 39.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2955.12it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 266kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.2MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 76.5MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 38.9MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.36it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2554.39it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 1.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 43.6MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 80.4MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 36.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2755.18it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 356658.50it/s]\n",
      "21626it [00:00, 337129.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on kick\n",
      "Test accuaracy: 0.7072967724035882\n",
      "Test accuaracy (sample): 0.65\n",
      "Test AUC: 0.7491010899483486\n",
      "Test AUC (sample): 0.7211497211497212\n",
      "Test % by label: [0.680384722093776, 0.319615277906224]\n",
      "Test % by label (sample): [0.63, 0.37]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"kick\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(\n",
    "        f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jigsaw (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 803kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 61.6MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 90.5MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 40.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:03<00:00,  1.09s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2733.63it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 573kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 58.5MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 86.8MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 44.7MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2793.72it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 440578.15it/s]\n",
      "25000it [00:00, 181158.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on jigsaw\n",
      "Test accuaracy: 0.94256\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9321638495642937\n",
      "Test AUC (sample): 0.8378947368421054\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 767kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 63.4MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 84.6MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 41.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.24it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2777.07it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 768kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 57.3MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 87.4MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 40.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.16it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2936.50it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 440115.84it/s]\n",
      "25000it [00:00, 469621.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on jigsaw\n",
      "Test accuaracy: 0.9448\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9456286008527085\n",
      "Test AUC (sample): 0.8631578947368422\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 737kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 58.7MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 87.3MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 40.5MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2961.38it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 723kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 56.8MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 85.9MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 41.7MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.24it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2653.50it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 447631.16it/s]\n",
      "25000it [00:00, 181678.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on jigsaw\n",
      "Test accuaracy: 0.9608\n",
      "Test accuaracy (sample): 0.96\n",
      "Test AUC: 0.9511750950554884\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.76k/1.76k [00:00<00:00, 762kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.75 MiB, generated: 61.00 MiB, post-processed: Unknown size, total: 88.75 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.83M/5.83M [00:00<00:00, 61.7MB/s]\n",
      "Downloading data: 100%|██████████| 19.8M/19.8M [00:00<00:00, 85.3MB/s]\n",
      "Downloading data: 100%|██████████| 3.49M/3.49M [00:00<00:00, 38.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2887.31it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.76k/1.76k [00:00<00:00, 598kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.75 MiB, generated: 61.00 MiB, post-processed: Unknown size, total: 88.75 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.83M/5.83M [00:00<00:00, 62.4MB/s]\n",
      "Downloading data: 100%|██████████| 19.8M/19.8M [00:00<00:00, 85.4MB/s]\n",
      "Downloading data: 100%|██████████| 3.49M/3.49M [00:00<00:00, 41.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3007.39it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_text on jigsaw\n",
      "Test accuaracy: 0.00184\n",
      "Test accuaracy (sample): 0.0\n",
      "Test AUC: 0.5108651509539557\n",
      "Test AUC (sample): 0.6252631578947369\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 4.12MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 65.0MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 89.9MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 40.2MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3063.77it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 634kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 63.6MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 92.3MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 41.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3044.50it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 831kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 64.8MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 85.7MB/s]\n",
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 44.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2938.56it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 451485.90it/s]\n",
      "25000it [00:00, 184091.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on jigsaw\n",
      "Test accuaracy: 0.93208\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9158202495728693\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"jigsaw\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(\n",
    "        f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 268kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.44MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.63MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.37MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.10it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2489.69it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 266kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.47MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.59MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.61MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3010.99it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 884874.26it/s]\n",
      "1273it [00:00, 1164271.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on prod_sent\n",
      "Test accuaracy: 0.8908091123330715\n",
      "Test accuaracy (sample): 0.95\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 848kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.57MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 10.8MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 7.40MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2511.06it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 269kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.35MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 10.3MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.61MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3006.67it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 953250.91it/s]\n",
      "1273it [00:00, 1418530.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on prod_sent\n",
      "Test accuaracy: 0.8695993715632364\n",
      "Test accuaracy (sample): 0.9\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 263kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.47MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.79MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 7.81MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.37it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2860.40it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 247kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 7.30MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.59MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 7.39MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3334.99it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 873813.33it/s]\n",
      "1273it [00:00, 994847.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on prod_sent\n",
      "Test accuaracy: 0.6614296936370778\n",
      "Test accuaracy (sample): 0.64\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 271kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 430.99 KiB, generated: 758.28 KiB, post-processed: Unknown size, total: 1.16 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.4k/90.4k [00:00<00:00, 5.87MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.67MB/s]]\n",
      "Downloading data: 100%|██████████| 54.1k/54.1k [00:00<00:00, 5.67MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2648.48it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 249kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 430.99 KiB, generated: 758.28 KiB, post-processed: Unknown size, total: 1.16 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.4k/90.4k [00:00<00:00, 5.93MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.58MB/s]]\n",
      "Downloading data: 100%|██████████| 54.1k/54.1k [00:00<00:00, 5.58MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2466.75it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_text on prod_sent\n",
      "Test accuaracy: 0.002356637863315004\n",
      "Test accuaracy (sample): 0.0\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 236kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.58MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.67MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 7.82MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2697.88it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 262kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 6.05MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.70MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 7.71MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2605.70it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 287kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 6.04MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.62MB/s]]\n",
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.85MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.37it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2957.21it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 940426.91it/s]\n",
      "1273it [00:00, 1408427.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on prod_sent\n",
      "Test accuaracy: 0.6496465043205027\n",
      "Test accuaracy (sample): 0.68\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"prod_sent\"\n",
    "\n",
    "for model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\", \"all_text\", \"stack\"]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    # print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    # print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(\n",
    "        f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def run_model(model_type, ds_type, test_set_size=100, tab_scale_factor=2):\n",
    "    di = get_dataset_info(ds_type, model_type)\n",
    "    # Data\n",
    "    train_df = load_dataset(\n",
    "        di.ds_name,\n",
    "        split=\"train\",  # download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name,\n",
    "        split=\"test\",  # download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df_sample = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if model_type == \"all_text\":\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            # model=di.text_model_name,\n",
    "            model=\"../models/wine/glowing-morning-9/checkpoint-6705\",\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert all columns to a single string\n",
    "\n",
    "        def cols_to_str_fn(array):\n",
    "            return \" | \".join(\n",
    "                [f\"{col}: {val}\" for col, val in zip(di.tab_cols + di.text_cols, array)]\n",
    "            )\n",
    "\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "            # cols=di.tab_cols + di.text_cols\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(di.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(di.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\"category\")\n",
    "        test_df_sample[di.categorical_cols] = test_df_sample[\n",
    "            di.categorical_cols\n",
    "        ].astype(\"category\")\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42)\n",
    "        tab_model.fit(train_df[di.tab_cols], y_train)\n",
    "\n",
    "        if model_type == \"ensemble_50\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.5,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_75\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.75,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_25\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.25,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                di.ds_name, split=\"validation\", download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\"category\")\n",
    "            y_val = val_df[di.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            # text_val_preds = np.array(\n",
    "            #     [format_text_pred(pred) for pred in text_val_preds]\n",
    "            # )\n",
    "            text_val_preds = np.array(\n",
    "                [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[di.tab_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(stack_val_df)\n",
    "            for i in range(text_val_preds.shape[1]):\n",
    "                stack_val_df[f\"text_pred_{i}\"] = text_val_preds[:, i]\n",
    "            for i in range(tab_val_preds.shape[1]):\n",
    "                stack_val_df[f\"tab_pred_{i}\"] = tab_val_preds[:, i]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(random_state=42)\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    test_sample_vals = test_df_sample[di.tab_cols + di.text_cols].values\n",
    "    test_vals = test_df[di.tab_cols + di.text_cols].iloc[:10].values\n",
    "    # train_sample = train_df.sample(1000, random_state=55)\n",
    "    # train_sample_y = train_sample[di.label_col]\n",
    "    # train_sample_vals = train_sample[di.tab_cols + di.text_cols].values\n",
    "    # return model.predict(train_sample_vals), train_sample_y\n",
    "    return model.predict(test_vals), test_df[di.label_col].iloc[:10].values\n",
    "    return (\n",
    "        model.predict(test_sample_vals),\n",
    "        test_df_sample[di.label_col].values,\n",
    "        model.predict(test_vals),\n",
    "        test_df[di.label_col].values,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def run_model(model_type, ds_type, test_set_size=100, tab_scale_factor=2):\n",
    "    di = get_dataset_info(ds_type, model_type)\n",
    "    # Data\n",
    "    train_df = load_dataset(\n",
    "        di.ds_name, split=\"train\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name, split=\"test\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df_sample = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if model_type == \"all_text\":\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            # model=di.text_model_name,\n",
    "            model=\"../models/wine/glowing-morning-9/checkpoint-6705\",\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert all columns to a single string\n",
    "\n",
    "        def cols_to_str_fn(array):\n",
    "            return \" | \".join(\n",
    "                [f\"{col}: {val}\" for col, val in zip(\n",
    "                    di.tab_cols + di.text_cols, array)]\n",
    "            )\n",
    "\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "            # cols=di.tab_cols + di.text_cols\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(di.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(di.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\n",
    "            \"category\")\n",
    "        test_df_sample[di.categorical_cols] = test_df_sample[\n",
    "            di.categorical_cols\n",
    "        ].astype(\"category\")\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42)\n",
    "        tab_model.fit(train_df[di.tab_cols], y_train)\n",
    "\n",
    "        if model_type == \"ensemble_50\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.5,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_75\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.75,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_25\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.25,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                di.ds_name, split=\"validation\", download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\n",
    "                \"category\")\n",
    "            y_val = val_df[di.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            # text_val_preds = np.array(\n",
    "            #     [format_text_pred(pred) for pred in text_val_preds]\n",
    "            # )\n",
    "            text_val_preds = np.array(\n",
    "                [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[di.tab_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(stack_val_df)\n",
    "            for i in range(text_val_preds.shape[1]):\n",
    "                stack_val_df[f\"text_pred_{i}\"] = text_val_preds[:, i]\n",
    "            for i in range(tab_val_preds.shape[1]):\n",
    "                stack_val_df[f\"tab_pred_{i}\"] = tab_val_preds[:, i]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(random_state=42)\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    test_sample_vals = test_df_sample[di.tab_cols + di.text_cols].values\n",
    "    test_vals = test_df[di.tab_cols + di.text_cols].values\n",
    "\n",
    "    return (\n",
    "        model.predict(test_sample_vals),\n",
    "        test_df_sample[di.label_col].values,\n",
    "        model.predict(test_vals),\n",
    "        test_df[di.label_col].values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset wine, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 673/673 [00:00<00:00, 261kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 15.47 MiB, generated: 29.51 MiB, post-processed: Unknown size, total: 44.99 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.24M/3.24M [00:00<00:00, 47.9MB/s]\n",
      "Downloading data: 100%|██████████| 11.0M/11.0M [00:00<00:00, 75.8MB/s]\n",
      "Downloading data: 100%|██████████| 1.95M/1.95M [00:00<00:00, 34.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.06it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2845.53it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 673/673 [00:00<00:00, 299kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 15.47 MiB, generated: 29.51 MiB, post-processed: Unknown size, total: 44.99 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.24M/3.24M [00:00<00:00, 38.6MB/s]\n",
      "Downloading data: 100%|██████████| 11.0M/11.0M [00:00<00:00, 72.8MB/s]\n",
      "Downloading data: 100%|██████████| 1.95M/1.95M [00:00<00:00, 37.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.18it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2864.31it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 648269.55it/s]\n",
      "21031it [00:00, 541480.41it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwine\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m      4\u001b[0m     \u001b[39m# \"all_text\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mensemble_25\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39m#     model_type, ds_type=ds_type, tab_scale_factor=1\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     preds, y \u001b[39m=\u001b[39m run_model(model_type, ds_type\u001b[39m=\u001b[39mds_type, tab_scale_factor\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mds_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest accuaracy: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39margmax(preds,\u001b[39m \u001b[39maxis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m \u001b[39m\u001b[39m==\u001b[39m\u001b[39m \u001b[39my)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "ds_type = \"wine\"\n",
    "\n",
    "for model_type in [\n",
    "    # \"all_text\",\n",
    "    \"ensemble_25\",\n",
    "    #    \"ensemble_50\",\n",
    "    #    \"ensemble_75\",\n",
    "    #    \"stack\"\n",
    "]:\n",
    "    # sample_preds, sample_y, preds, y = run_model(\n",
    "    #     model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    # )\n",
    "    preds, y = run_model(model_type, ds_type=ds_type, tab_scale_factor=1)\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    # print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    # print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 29,  5, ...,  5,  3, 26])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  9, 23, ..., 23,  2, 15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
