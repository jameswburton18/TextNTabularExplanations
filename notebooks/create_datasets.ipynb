{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from auto_mm_bench.datasets import dataset_registry\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from src.utils import get_dataset_info\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.                                                          \n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 3865.72it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4328.49it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 29746.84it/s]\n",
      "Pushing split train to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 27235.74it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 27594.11it/s]\n",
      "Pushing split train to the Hub.                                          \n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 27413.75it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 27962.03it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4963.67it/s]\n",
      "Downloading metadata: 100%|██████████| 716/716 [00:00<00:00, 199kB/s]\n",
      "Pushing split train to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 263.20ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 246.25ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "Pushing split test to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 296.21ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Pushing split train to the Hub.                                         \n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4202.71it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 28532.68it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4262.50it/s]\n",
      "Downloading metadata: 100%|██████████| 595/595 [00:00<00:00, 597kB/s]\n",
      "Pushing split train to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 27962.03it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4293.04it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "Pushing split train to the Hub.                                                          \n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 3837.42it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 16256.99it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "Downloading metadata: 100%|██████████| 847/847 [00:00<00:00, 380kB/s]\n",
      "Pushing split train to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4481.09it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4378.19it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 29537.35it/s]\n",
      "Pushing split train to the Hub.                                                           \n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 27594.11it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4369.07it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4315.13it/s]\n",
      "Downloading metadata: 100%|██████████| 1.76k/1.76k [00:00<00:00, 562kB/s]\n",
      "Pushing split train to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4573.94it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 30174.85it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4297.44it/s]\n",
      "Pushing split train to the Hub.                                        \n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 19152.07it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 26715.31it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4052.47it/s]\n",
      "Downloading metadata: 100%|██████████| 911/911 [00:00<00:00, 400kB/s]\n",
      "Pushing split train to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 4452.55it/s]\n",
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00, 27413.75it/s]\n",
      "Downloading metadata: 100%|██████████| 904/904 [00:00<00:00, 1.14MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"product_sentiment_machine_hack\"\n",
    "# [\"wine_reviews\" ,\"fake_job_postings2\" , \"product_sentiment_machine_hack\", \"kick_starter_funding\", \"jigsaw_unintended_bias100K\"]\n",
    "\n",
    "for dataset_name in [\n",
    "    \"wine_reviews\",\n",
    "    \"fake_job_postings2\",\n",
    "    \"product_sentiment_machine_hack\",\n",
    "    \"kick_starter_funding\",\n",
    "    \"jigsaw_unintended_bias100K\",\n",
    "    \"imdb_genre_prediction\",\n",
    "]:\n",
    "    di = get_dataset_info(dataset_name)\n",
    "    train_dataset = dataset_registry.create(dataset_name, \"train\")\n",
    "    test_dataset = dataset_registry.create(dataset_name, \"test\")\n",
    "    cols = train_dataset.feature_columns + train_dataset.label_columns\n",
    "\n",
    "    train_txt = train_dataset.data[cols]\n",
    "    test_txt = test_dataset.data[cols]\n",
    "\n",
    "    # # Fill missing values with \"None\"\n",
    "    # train_txt = train_txt.fillna(\"None\")\n",
    "    # test_txt = test_txt.fillna(\"None\")\n",
    "\n",
    "    # load dataset from dataframe\n",
    "    train_ds = Dataset.from_pandas(train_txt)\n",
    "    train_ds = train_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "    test_ds = Dataset.from_pandas(test_txt)\n",
    "    test_ds = test_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "\n",
    "    train_ds = train_ds.train_test_split(\n",
    "        test_size=0.15, seed=42, stratify_by_column=train_dataset.label_columns[0]\n",
    "    )\n",
    "\n",
    "    ds = DatasetDict(\n",
    "        {\"train\": train_ds[\"train\"],\n",
    "            \"validation\": train_ds[\"test\"], \"test\": test_ds}\n",
    "    )\n",
    "\n",
    "    # Now we have made the split but still need to deal with missing values, and that depends on the column type\n",
    "\n",
    "    # All as text\n",
    "    train_all_text = ds[\"train\"].to_pandas()\n",
    "    val_all_text = ds[\"validation\"].to_pandas()\n",
    "    test_all_text = ds[\"test\"].to_pandas()\n",
    "\n",
    "    train_all_text[train_dataset.feature_columns] = train_all_text[\n",
    "        train_dataset.feature_columns\n",
    "    ].astype(\"str\")\n",
    "    val_all_text[train_dataset.feature_columns] = val_all_text[\n",
    "        train_dataset.feature_columns\n",
    "    ].astype(\"str\")\n",
    "    test_all_text[train_dataset.feature_columns] = test_all_text[\n",
    "        train_dataset.feature_columns\n",
    "    ].astype(\"str\")\n",
    "\n",
    "    ds_all_text = DatasetDict(\n",
    "        {\n",
    "            \"train\": Dataset.from_pandas(train_all_text),\n",
    "            \"validation\": Dataset.from_pandas(val_all_text),\n",
    "            \"test\": Dataset.from_pandas(test_all_text),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds_all_text.push_to_hub(dataset_name + \"_all_text\")\n",
    "\n",
    "    # Not all as text\n",
    "    train = ds[\"train\"].to_pandas()\n",
    "    val = ds[\"validation\"].to_pandas()\n",
    "    test = ds[\"test\"].to_pandas()\n",
    "\n",
    "    train[di.text_cols] = train[di.text_cols].astype(\"str\")\n",
    "    val[di.text_cols] = val[di.text_cols].astype(\"str\")\n",
    "    test[di.text_cols] = test[di.text_cols].astype(\"str\")\n",
    "\n",
    "    # ds.push_to_hub(dataset_name)\n",
    "    if len(di.categorical_cols) > 0:\n",
    "        train[di.categorical_cols] = train[di.categorical_cols].astype(\n",
    "            \"category\")\n",
    "\n",
    "        enc = OrdinalEncoder(encoded_missing_value=-1)\n",
    "        train[di.categorical_cols] = enc.fit_transform(\n",
    "            train[di.categorical_cols])\n",
    "\n",
    "        val[di.categorical_cols] = val[di.categorical_cols].astype(\"category\")\n",
    "        val[di.categorical_cols] = enc.transform(val[di.categorical_cols])\n",
    "\n",
    "        test[di.categorical_cols] = test[di.categorical_cols].astype(\n",
    "            \"category\")\n",
    "        test[di.categorical_cols] = enc.transform(test[di.categorical_cols])\n",
    "\n",
    "    ds2 = DatasetDict(\n",
    "        {\n",
    "            \"train\": Dataset.from_pandas(train),\n",
    "            \"validation\": Dataset.from_pandas(val),\n",
    "            \"test\": Dataset.from_pandas(test),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds2.push_to_hub(dataset_name + \"_ordinal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
