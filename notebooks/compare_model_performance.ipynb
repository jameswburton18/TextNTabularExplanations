{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from src.dataset_info import get_dataset_info\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.utils import token_segments, text_ft_index_ends\n",
    "\n",
    "# from src.models import Model\n",
    "import lightgbm as lgb\n",
    "from src.models import WeightedEnsemble, StackModel, AllAsTextModel\n",
    "from src.joint_masker import JointMasker\n",
    "import argparse\n",
    "import scipy as sp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_type, ds_type, test_set_size=100, tab_scale_factor=2):\n",
    "    di = get_dataset_info(ds_type, model_type)\n",
    "    # Data\n",
    "    train_df = load_dataset(\n",
    "        di.ds_name, split=\"train\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[di.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        di.ds_name, split=\"test\", download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    test_df_sample = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    if model_type in [\n",
    "        \"all_text\",\n",
    "        \"all_as_text_tnt_reorder\",\n",
    "        \"all_as_text_base_reorder\",\n",
    "    ]:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert all columns to a single string\n",
    "        if model_type == \"all_text\":\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [\n",
    "                        f\"{col}: {val}\"\n",
    "                        for col, val in zip(di.tab_cols + di.text_cols, array)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        elif model_type == \"all_as_text_tnt_reorder\":\n",
    "            # Reorder based on the new index order in di\n",
    "            def cols_to_str_fn(array):\n",
    "                original = di.tab_cols + di.text_cols\n",
    "                new = di.tnt_reorder_cols\n",
    "                new_array = np.array([array[original.index(x)] for x in new])\n",
    "                return \" | \".join([f\"{col}: {val}\" for col, val in zip(new, new_array)])\n",
    "\n",
    "        else:\n",
    "            # Reorder based on the new index order in di\n",
    "            def cols_to_str_fn(array):\n",
    "                original = di.tab_cols + di.text_cols\n",
    "                new = di.base_reorder_cols\n",
    "                new_array = np.array([array[original.index(x)] for x in new])\n",
    "                return \" | \".join([f\"{col}: {val}\" for col, val in zip(new, new_array)])\n",
    "\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "            # cols=di.tab_cols + di.text_cols\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=di.text_model_name,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(di.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(di.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\"category\")\n",
    "        test_df_sample[di.categorical_cols] = test_df_sample[\n",
    "            di.categorical_cols\n",
    "        ].astype(\"category\")\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42)\n",
    "        tab_model.fit(train_df[di.tab_cols], y_train)\n",
    "\n",
    "        if model_type == \"ensemble_50\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.5,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_75\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.75,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"ensemble_25\":\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=0.25,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                di.ds_name, split=\"validation\", download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\"category\")\n",
    "            y_val = val_df[di.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            # text_val_preds = np.array(\n",
    "            #     [format_text_pred(pred) for pred in text_val_preds]\n",
    "            # )\n",
    "            text_val_preds = np.array(\n",
    "                [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[di.tab_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(stack_val_df)\n",
    "            for i in range(text_val_preds.shape[1]):\n",
    "                stack_val_df[f\"text_pred_{i}\"] = text_val_preds[:, i]\n",
    "            for i in range(tab_val_preds.shape[1]):\n",
    "                stack_val_df[f\"tab_pred_{i}\"] = tab_val_preds[:, i]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(random_state=42)\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    test_sample_vals = test_df_sample[di.tab_cols + di.text_cols].values\n",
    "    test_vals = test_df[di.tab_cols + di.text_cols].values\n",
    "\n",
    "    return (\n",
    "        model.predict(test_sample_vals),\n",
    "        test_df_sample[di.label_col].values,\n",
    "        model.predict(test_vals),\n",
    "        test_df[di.label_col].values,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Job Postings (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset fake, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 219kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 27.7MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 72.5MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 41.5MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2876.75it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 717/717 [00:00<00:00, 250kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 11.54 MiB, generated: 20.39 MiB, post-processed: Unknown size, total: 31.94 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 27.7MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 65.8MB/s]\n",
      "Downloading data: 100%|██████████| 2.46M/2.46M [00:00<00:00, 41.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.16it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2955.82it/s]\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--fake_job_postings2_ordinal-d873cc356e36f3d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 69626.56it/s]\n",
      "3182it [00:00, 54455.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfake\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mensemble_25\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mensemble_50\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# \"all_as_text_base_reorder\",\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ]:\n\u001b[0;32m---> 12\u001b[0m     sample_preds, sample_y, preds, y \u001b[39m=\u001b[39m run_model(\n\u001b[1;32m     13\u001b[0m         model_type, ds_type\u001b[39m=\u001b[39;49mds_type, tab_scale_factor\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mds_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest accuaracy: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39margmax(preds,\u001b[39m \u001b[39maxis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m \u001b[39m\u001b[39m==\u001b[39m\u001b[39m \u001b[39my)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 168\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model_type, ds_type, test_set_size, tab_scale_factor)\u001b[0m\n\u001b[1;32m    162\u001b[0m test_sample_vals \u001b[39m=\u001b[39m test_df_sample[di\u001b[39m.\u001b[39mtab_cols \u001b[39m+\u001b[39m di\u001b[39m.\u001b[39mtext_cols]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m    163\u001b[0m test_vals \u001b[39m=\u001b[39m test_df[di\u001b[39m.\u001b[39mtab_cols \u001b[39m+\u001b[39m di\u001b[39m.\u001b[39mtext_cols]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    166\u001b[0m     model\u001b[39m.\u001b[39mpredict(test_sample_vals),\n\u001b[1;32m    167\u001b[0m     test_df_sample[di\u001b[39m.\u001b[39mlabel_col]\u001b[39m.\u001b[39mvalues,\n\u001b[0;32m--> 168\u001b[0m     model\u001b[39m.\u001b[39;49mpredict(test_vals),\n\u001b[1;32m    169\u001b[0m     test_df[di\u001b[39m.\u001b[39mlabel_col]\u001b[39m.\u001b[39mvalues,\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/src/models.py:46\u001b[0m, in \u001b[0;36mWeightedEnsemble.predict\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     44\u001b[0m dict_keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(desc_dict\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m     45\u001b[0m dict_keys \u001b[39m=\u001b[39m dict_keys[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dict_keys) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m dict_keys\n\u001b[0;32m---> 46\u001b[0m text_preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_pipeline(dict_keys)\n\u001b[1;32m     47\u001b[0m text_preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([format_text_pred(pred) \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m text_preds])\n\u001b[1;32m     49\u001b[0m expanded_text_preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(text_examples), text_preds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    156\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1065\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1062\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1063\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1064\u001b[0m     )\n\u001b[0;32m-> 1065\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1067\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1065\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1062\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1063\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1064\u001b[0m     )\n\u001b[0;32m-> 1065\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1067\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/pipelines/base.py:992\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    991\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 992\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    993\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    994\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:182\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:759\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 759\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    760\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    761\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    762\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    763\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    764\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    765\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    766\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    768\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:579\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    580\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    581\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    582\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    583\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    584\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    585\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    586\u001b[0m )\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:355\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    353\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 355\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    356\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    357\u001b[0m )\n\u001b[1;32m    358\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:290\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    291\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    292\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    293\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    294\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    295\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    296\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    297\u001b[0m )\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    299\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:216\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    214\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    217\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[1;32m    218\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[1;32m    219\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    221\u001b[0m weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds_type = \"fake\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    # \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB  (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 320kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 5.31MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 27.2MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.23MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2772.18it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 830kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.32MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 42.6MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.78MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.59it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2697.30it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 611414.58it/s]\n",
      "200it [00:00, 750994.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on imdb_genre\n",
      "Test accuaracy: 0.78\n",
      "Test accuaracy (sample): 0.79\n",
      "Test AUC: 0.8530677609848865\n",
      "Test AUC (sample): 0.8262626262626263\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 976kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.50MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 27.1MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 8.04MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2730.07it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 261kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.50MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 34.3MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.14MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2673.23it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 747647.77it/s]\n",
      "200it [00:00, 821607.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on imdb_genre\n",
      "Test accuaracy: 0.785\n",
      "Test accuaracy (sample): 0.79\n",
      "Test AUC: 0.8646782103893504\n",
      "Test AUC (sample): 0.8513131313131314\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 471kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.82MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.4MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.86MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2734.82it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 871kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.72MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 36.7MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 8.06MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2499.09it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 618628.91it/s]\n",
      "200it [00:00, 757778.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on imdb_genre\n",
      "Test accuaracy: 0.775\n",
      "Test accuaracy (sample): 0.75\n",
      "Test AUC: 0.8520668601741568\n",
      "Test AUC (sample): 0.8387878787878787\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 510kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.22MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 34.3MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 8.07MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2655.18it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 333kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.22MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 27.9MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 6.97MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2577.94it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 1.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 8.34MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 28.9MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.63MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.68it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2450.42it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 584164.90it/s]\n",
      "200it [00:00, 976555.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on imdb_genre\n",
      "Test accuaracy: 0.74\n",
      "Test accuaracy (sample): 0.73\n",
      "Test AUC: 0.8106295666099489\n",
      "Test AUC (sample): 0.7846464646464647\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 906/906 [00:00<00:00, 315kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 225.28 KiB, generated: 320.45 KiB, post-processed: Unknown size, total: 545.73 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.0k/51.0k [00:00<00:00, 1.02MB/s]\n",
      "Downloading data: 100%|██████████| 34.4k/34.4k [00:00<00:00, 37.5MB/s]\n",
      "Downloading data: 100%|██████████| 145k/145k [00:00<00:00, 6.95MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2971.87it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 906/906 [00:00<00:00, 1.09MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 225.28 KiB, generated: 320.45 KiB, post-processed: Unknown size, total: 545.73 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.0k/51.0k [00:00<00:00, 5.41MB/s]\n",
      "Downloading data: 100%|██████████| 34.4k/34.4k [00:00<00:00, 32.4MB/s]\n",
      "Downloading data: 100%|██████████| 145k/145k [00:00<00:00, 8.24MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.44it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2818.12it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_all_text-e7768922c61ebd55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_text on imdb_genre\n",
      "Test accuaracy: 0.72\n",
      "Test accuaracy (sample): 0.69\n",
      "Test AUC: 0.8146331698528675\n",
      "Test AUC (sample): 0.8004040404040405\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 344kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 6.85MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.4MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 8.03MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2789.38it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 282kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 5.37MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 38.1MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.92MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.45it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2451.85it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_as_text_tnt_reorder on imdb_genre\n",
      "Test accuaracy: 0.735\n",
      "Test accuaracy (sample): 0.73\n",
      "Test AUC: 0.816634971474327\n",
      "Test AUC (sample): 0.8311111111111111\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n",
      "Using dataset imdb_genre, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 359kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 5.36MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 33.9MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 7.32MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2922.18it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 904/904 [00:00<00:00, 346kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 224.75 KiB, generated: 322.65 KiB, post-processed: Unknown size, total: 547.40 KiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.1k/51.1k [00:00<00:00, 7.95MB/s]\n",
      "Downloading data: 100%|██████████| 34.6k/34.6k [00:00<00:00, 39.6MB/s]\n",
      "Downloading data: 100%|██████████| 144k/144k [00:00<00:00, 8.02MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2762.44it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--imdb_genre_prediction_ordinal-95c476e18d2d7064/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "\n",
      "all_as_text_base_reorder on imdb_genre\n",
      "Test accuaracy: 0.735\n",
      "Test accuaracy (sample): 0.73\n",
      "Test AUC: 0.8170353317986188\n",
      "Test AUC (sample): 0.8335353535353536\n",
      "Test % by label: [0.485, 0.515]\n",
      "Test % by label (sample): [0.55, 0.45]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"imdb_genre\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kickstarter  (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset kick, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 864kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 29.9MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 26.8MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 35.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.34it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1915.50it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 841/841 [00:00<00:00, 582kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 17.58 MiB, generated: 29.54 MiB, post-processed: Unknown size, total: 47.11 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.69M/3.69M [00:00<00:00, 29.9MB/s]\n",
      "Downloading data: 100%|██████████| 2.22M/2.22M [00:00<00:00, 24.1MB/s]\n",
      "Downloading data: 100%|██████████| 12.5M/12.5M [00:00<00:00, 30.0MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1787.85it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--kick_starter_funding_ordinal-bc45dae77b1c676d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkick\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m      4\u001b[0m     \u001b[39m# \"ensemble_25\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# \"ensemble_50\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mall_as_text_base_reorder\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]:\n\u001b[0;32m---> 12\u001b[0m     sample_preds, sample_y, preds, y \u001b[39m=\u001b[39m run_model(\n\u001b[1;32m     13\u001b[0m         model_type, ds_type\u001b[39m=\u001b[39;49mds_type, tab_scale_factor\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mds_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest accuaracy: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39margmax(preds,\u001b[39m \u001b[39maxis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m \u001b[39m\u001b[39m==\u001b[39m\u001b[39m \u001b[39my)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 168\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model_type, ds_type, test_set_size, tab_scale_factor)\u001b[0m\n\u001b[1;32m    162\u001b[0m test_sample_vals \u001b[39m=\u001b[39m test_df_sample[di\u001b[39m.\u001b[39mtab_cols \u001b[39m+\u001b[39m di\u001b[39m.\u001b[39mtext_cols]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m    163\u001b[0m test_vals \u001b[39m=\u001b[39m test_df[di\u001b[39m.\u001b[39mtab_cols \u001b[39m+\u001b[39m di\u001b[39m.\u001b[39mtext_cols]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    166\u001b[0m     model\u001b[39m.\u001b[39mpredict(test_sample_vals),\n\u001b[1;32m    167\u001b[0m     test_df_sample[di\u001b[39m.\u001b[39mlabel_col]\u001b[39m.\u001b[39mvalues,\n\u001b[0;32m--> 168\u001b[0m     model\u001b[39m.\u001b[39;49mpredict(test_vals),\n\u001b[1;32m    169\u001b[0m     test_df[di\u001b[39m.\u001b[39mlabel_col]\u001b[39m.\u001b[39mvalues,\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/CodingProjects/TextNTabularExplanations/src/models.py:117\u001b[0m, in \u001b[0;36mAllAsTextModel.predict\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, examples):\n\u001b[1;32m    114\u001b[0m     \u001b[39m# examples_as_strings = np.apply_along_axis(\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39m#     lambda x: array_to_string(x, self.cols), 1, examples\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     examples_as_strings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcols_to_str_fn, examples)))\n\u001b[1;32m    118\u001b[0m     preds \u001b[39m=\u001b[39m [\n\u001b[1;32m    119\u001b[0m         out\n\u001b[1;32m    120\u001b[0m         \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_pipeline(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    124\u001b[0m     ]\n\u001b[1;32m    125\u001b[0m     preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([format_text_pred(pred) \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m preds])\n",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m, in \u001b[0;36mrun_model.<locals>.cols_to_str_fn\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m     45\u001b[0m new \u001b[39m=\u001b[39m di\u001b[39m.\u001b[39mtnt_reorder_cols\n\u001b[1;32m     46\u001b[0m new_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([array[original\u001b[39m.\u001b[39mindex(x)] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m new])\n\u001b[0;32m---> 47\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39;49m\u001b[39m | \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(new_array)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "ds_type = \"kick\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jigsaw (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 482kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 6.56MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 22.9MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 60.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1978.45it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 2.13MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 42.0MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 86.1MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 58.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2778.91it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 358487.52it/s]\n",
      "25000it [00:00, 169892.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on jigsaw\n",
      "Test accuaracy: 0.94256\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9321638495642937\n",
      "Test AUC (sample): 0.8378947368421054\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 1.81MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 40.7MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 81.9MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 57.3MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2931.03it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 555kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 41.7MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 92.4MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 61.7MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2933.08it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 440115.84it/s]\n",
      "25000it [00:00, 466658.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on jigsaw\n",
      "Test accuaracy: 0.9448\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9456286008527085\n",
      "Test AUC (sample): 0.8631578947368422\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 1.96MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 42.4MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 86.3MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 61.0MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2820.02it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 2.34MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 41.6MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 87.4MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 62.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2864.31it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 405638.68it/s]\n",
      "25000it [00:00, 347226.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on jigsaw\n",
      "Test accuaracy: 0.9608\n",
      "Test accuaracy (sample): 0.96\n",
      "Test AUC: 0.9511750950554884\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.76k/1.76k [00:00<00:00, 593kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.75 MiB, generated: 61.00 MiB, post-processed: Unknown size, total: 88.75 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.49M/3.49M [00:00<00:00, 41.4MB/s]\n",
      "Downloading data: 100%|██████████| 19.8M/19.8M [00:00<00:00, 87.8MB/s]\n",
      "Downloading data: 100%|██████████| 5.83M/5.83M [00:00<00:00, 63.0MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.34it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2605.70it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.76k/1.76k [00:00<00:00, 2.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.75 MiB, generated: 61.00 MiB, post-processed: Unknown size, total: 88.75 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.49M/3.49M [00:00<00:00, 43.8MB/s]\n",
      "Downloading data: 100%|██████████| 19.8M/19.8M [00:00<00:00, 86.2MB/s]\n",
      "Downloading data: 100%|██████████| 5.83M/5.83M [00:00<00:00, 61.8MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.23it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2644.02it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_all_text-351e9b1e029b8621/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:06<00:00, 42.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_text on jigsaw\n",
      "Test accuaracy: 0.9612\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9624062142594692\n",
      "Test AUC (sample): 0.9494736842105264\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n",
      "Using dataset jigsaw, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 2.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 41.9MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 84.2MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 56.4MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2563.23it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 650kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 46.7MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 85.2MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 64.1MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2744.36it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.77k/1.77k [00:00<00:00, 496kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 27.70 MiB, generated: 66.00 MiB, post-processed: Unknown size, total: 93.70 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.48M/3.48M [00:00<00:00, 40.9MB/s]\n",
      "Downloading data: 100%|██████████| 19.7M/19.7M [00:00<00:00, 84.2MB/s]\n",
      "Downloading data: 100%|██████████| 5.82M/5.82M [00:00<00:00, 58.2MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3000.22it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--jigsaw_unintended_bias100K_ordinal-8e97391c4f489562/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 430185.03it/s]\n",
      "25000it [00:00, 446485.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on jigsaw\n",
      "Test accuaracy: 0.93208\n",
      "Test accuaracy (sample): 0.95\n",
      "Test AUC: 0.9158202495728693\n",
      "Test AUC (sample): 0.9515789473684211\n",
      "Test % by label: [0.9426, 0.0574]\n",
      "Test % by label (sample): [0.95, 0.05]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"jigsaw\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 489kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 6.48MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 1.27MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 595kB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2763.05it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 211kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.41MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 11.6MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 4.83MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2990.24it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 921825.05it/s]\n",
      "1273it [00:00, 1303233.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_25 on prod_sent\n",
      "Test accuaracy: 0.8908091123330715\n",
      "Test accuaracy (sample): 0.95\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 666kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.56MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.70MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.73MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2791.24it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 677kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.58MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.66MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.53MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.56it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2907.33it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 731990.23it/s]\n",
      "1273it [00:00, 1375411.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_50 on prod_sent\n",
      "Test accuaracy: 0.8695993715632364\n",
      "Test accuaracy (sample): 0.9\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 761kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.63MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.73MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.67MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2988.82it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 704kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.59MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 10.1MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.72MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2956.51it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 530924.56it/s]\n",
      "1273it [00:00, 1430310.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ensemble_75 on prod_sent\n",
      "Test accuaracy: 0.6614296936370778\n",
      "Test accuaracy (sample): 0.64\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 736kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 430.99 KiB, generated: 758.28 KiB, post-processed: Unknown size, total: 1.16 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.1k/54.1k [00:00<00:00, 8.07MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 10.0MB/s]]\n",
      "Downloading data: 100%|██████████| 90.4k/90.4k [00:00<00:00, 5.61MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2615.99it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 197kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 430.99 KiB, generated: 758.28 KiB, post-processed: Unknown size, total: 1.16 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.1k/54.1k [00:00<00:00, 5.69MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.85MB/s]]\n",
      "Downloading data: 100%|██████████| 90.4k/90.4k [00:00<00:00, 5.66MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2977.50it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_all_text-64fff8d5159768bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:02<00:00, 108MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_text on prod_sent\n",
      "Test accuaracy: 0.8476040848389631\n",
      "Test accuaracy (sample): 0.88\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n",
      "Using dataset prod_sent, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 448kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 7.92MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.80MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.66MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2966.97it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 632kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.55MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.79MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 6.21MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2580.58it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 590/590 [00:00<00:00, 216kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 431.83 KiB, generated: 777.70 KiB, post-processed: Unknown size, total: 1.18 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 54.2k/54.2k [00:00<00:00, 5.76MB/s]\n",
      "Downloading data: 100%|██████████| 297k/297k [00:00<00:00, 9.82MB/s]]\n",
      "Downloading data: 100%|██████████| 90.6k/90.6k [00:00<00:00, 5.73MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2945.44it/s]\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--product_sentiment_machine_hack_ordinal-bbe910d41184c5e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100it [00:00, 907858.01it/s]\n",
      "1273it [00:00, 1387204.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stack on prod_sent\n",
      "Test accuaracy: 0.6496465043205027\n",
      "Test accuaracy (sample): 0.68\n",
      "Test % by label: [0.010997643362136685, 0.0589159465828751, 0.5875883739198743, 0.3424980361351139]\n",
      "Test % by label (sample): [0.01, 0.02, 0.62, 0.35]\n"
     ]
    }
   ],
   "source": [
    "ds_type = \"prod_sent\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    # \"all_as_text_tnt_reorder\",\n",
    "    # \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    sample_preds, sample_y, preds, y = run_model(\n",
    "        model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    # print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    # print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset wine, ordinal version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 673/673 [00:00<00:00, 721kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 15.47 MiB, generated: 29.51 MiB, post-processed: Unknown size, total: 44.99 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.95M/1.95M [00:00<00:00, 4.27MB/s]\n",
      "Downloading data: 100%|██████████| 11.0M/11.0M [00:00<00:00, 15.9MB/s]\n",
      "Downloading data: 100%|██████████| 3.24M/3.24M [00:00<00:00, 6.04MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:04<00:00,  1.67s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1963.63it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 673/673 [00:00<00:00, 410kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 15.47 MiB, generated: 29.51 MiB, post-processed: Unknown size, total: 44.99 MiB) to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.95M/1.95M [00:00<00:00, 36.0MB/s]\n",
      "Downloading data: 100%|██████████| 11.0M/11.0M [00:00<00:00, 72.1MB/s]\n",
      "Downloading data: 100%|██████████| 3.24M/3.24M [00:00<00:00, 43.6MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2816.23it/s]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--wine_reviews_ordinal-c036c1f97c7da848/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 645277.54it/s]\n",
      "21031it [00:00, 470854.79it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwine\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m      4\u001b[0m     \u001b[39m# \"all_text\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mensemble_25\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39m#     model_type, ds_type=ds_type, tab_scale_factor=1\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     preds, y \u001b[39m=\u001b[39m run_model(model_type, ds_type\u001b[39m=\u001b[39mds_type, tab_scale_factor\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mds_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest accuaracy: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39margmax(preds,\u001b[39m \u001b[39maxis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m \u001b[39m\u001b[39m==\u001b[39m\u001b[39m \u001b[39my)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "ds_type = \"wine\"\n",
    "\n",
    "for model_type in [\n",
    "    \"ensemble_25\",\n",
    "    \"ensemble_50\",\n",
    "    \"ensemble_75\",\n",
    "    \"stack\",\n",
    "    \"all_text\",\n",
    "    \"all_as_text_tnt_reorder\",\n",
    "    # \"all_as_text_base_reorder\",\n",
    "]:\n",
    "    # sample_preds, sample_y, preds, y = run_model(\n",
    "    #     model_type, ds_type=ds_type, tab_scale_factor=1\n",
    "    # )\n",
    "    preds, y = run_model(model_type, ds_type=ds_type, tab_scale_factor=1)\n",
    "\n",
    "    print(f\"\\n{model_type} on {ds_type}\")\n",
    "    print(f\"Test accuaracy: {np.mean(np.argmax(preds, axis=1) == y)}\")\n",
    "    print(\n",
    "        f\"Test accuaracy (sample): {np.mean(np.argmax(sample_preds, axis=1) == sample_y)}\"\n",
    "    )\n",
    "    # print(f\"Test AUC: {roc_auc_score(y, preds[:, 1])}\")\n",
    "    # print(f\"Test AUC (sample): {roc_auc_score(sample_y, sample_preds[:, 1])}\")\n",
    "    print(\n",
    "        f\"Test % by label: {[np.mean(y == label) for label in np.unique(y)]}\")\n",
    "    print(\n",
    "        f\"Test % by label (sample): {[np.mean(sample_y == label) for label in np.unique(y)]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 29,  5, ...,  5,  3, 26])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  9, 23, ..., 23,  2, 15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
