{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = [\n",
    "    \"accommodates\",\n",
    "    \"availability_30\",\n",
    "    \"availability_365\",\n",
    "    \"availability_60\",\n",
    "    \"availability_90\",\n",
    "    \"bathrooms\",\n",
    "    \"bedrooms\",\n",
    "    \"beds\",\n",
    "    \"calculated_host_listings_count\",\n",
    "    \"cleaning_fee\",\n",
    "    \"extra_people\",\n",
    "    \"guests_included\",\n",
    "    \"host_has_profile_pic\",\n",
    "    \"host_identity_verified\",\n",
    "    \"host_is_superhost\",\n",
    "    \"host_response_time\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"maximum_nights\",\n",
    "    \"minimum_nights\",\n",
    "    \"number_of_reviews\",\n",
    "    \"review_scores_accuracy\",\n",
    "    \"review_scores_checkin\",\n",
    "    \"review_scores_cleanliness\",\n",
    "    \"review_scores_communication\",\n",
    "    \"review_scores_location\",\n",
    "    \"review_scores_rating\",\n",
    "    \"review_scores_value\",\n",
    "    \"reviews_per_month\",\n",
    "    \"security_deposit\",\n",
    "]\n",
    "\n",
    "cat = [\n",
    "    \"bed_type\",\n",
    "    \"cancellation_policy\",\n",
    "    \"host_verifications_email\",\n",
    "    \"host_verifications_facebook\",\n",
    "    \"host_verifications_google\",\n",
    "    \"host_verifications_government_id\",\n",
    "    \"host_verifications_identity_manual\",\n",
    "    \"host_verifications_jumio\",\n",
    "    \"host_verifications_kba\",\n",
    "    \"host_verifications_manual_offline\",\n",
    "    \"host_verifications_manual_online\",\n",
    "    \"host_verifications_offline_government_id\",\n",
    "    \"host_verifications_phone\",\n",
    "    \"host_verifications_reviews\",\n",
    "    \"host_verifications_selfie\",\n",
    "    \"host_verifications_sent_id\",\n",
    "    \"host_verifications_sesame\",\n",
    "    \"host_verifications_sesame_offline\",\n",
    "    \"host_verifications_weibo\",\n",
    "    \"host_verifications_work_email\",\n",
    "    \"host_verifications_zhima_selfie\",\n",
    "    \"instant_bookable\",\n",
    "    \"is_location_exact\",\n",
    "    \"license\",\n",
    "    \"require_guest_phone_verification\",\n",
    "    \"require_guest_profile_picture\",\n",
    "    \"room_type\",\n",
    "]\n",
    "\n",
    "string = [\n",
    "    \"access\",\n",
    "    \"amenities\",\n",
    "    \"calendar_updated\",\n",
    "    \"city\",\n",
    "    \"description\",\n",
    "    \"first_review\",\n",
    "    \"host_about\",\n",
    "    \"host_location\",\n",
    "    \"host_neighborhood\",\n",
    "    \"host_response_rate\",\n",
    "    \"host_since\",\n",
    "    \"host_verifications\",\n",
    "    \"house_rules\",\n",
    "    \"interaction\",\n",
    "    \"last_review\",\n",
    "    \"name\",\n",
    "    \"neighborhood\",\n",
    "    \"neighborhood_overview\",\n",
    "    \"property_type\",\n",
    "    \"smart_location\",\n",
    "    \"space\",\n",
    "    \"state\",\n",
    "    \"street\",\n",
    "    \"suburb\",\n",
    "    \"summary\",\n",
    "    \"transit\",\n",
    "    \"zipcode\",\n",
    "]\n",
    "len(num + cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/TextNTabularExplanations/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from auto_mm_bench.datasets import dataset_registry\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from src.dataset_info import get_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import wandb\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from lion_pytorch import Lion\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim import AdamW\n",
    "from src.utils import (\n",
    "    prepare_text,\n",
    "    row_to_string,\n",
    "    multiple_row_to_string,\n",
    ")\n",
    "from src.dataset_info import get_dataset_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    }
   ],
   "source": [
    "dataset_name = \"melbourne_airbnb\"\n",
    "\n",
    "train_dataset = dataset_registry.create(dataset_name, \"train\")\n",
    "test_dataset = dataset_registry.create(dataset_name, \"test\")\n",
    "cols = train_dataset.feature_columns + train_dataset.label_columns\n",
    "\n",
    "train_txt = train_dataset.data[cols]\n",
    "test_txt = test_dataset.data[cols]\n",
    "\n",
    "# load dataset from dataframe\n",
    "train_ds = Dataset.from_pandas(train_txt)\n",
    "train_ds = train_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "test_ds = Dataset.from_pandas(test_txt)\n",
    "test_ds = test_ds.class_encode_column(train_dataset.label_columns[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model type specified for jigsaw_unintended_bias100K. (This is fine during dataset creation)\n",
      "No model type specified for jigsaw. (This is fine during dataset creation)                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset jigsaw_unintended_bias100K, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   0%|          | 0/85000 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "model_base = \"microsoft/deberta-v3-base\"\n",
    "dataset_name = \"jigsaw_unintended_bias100K\"\n",
    "\n",
    "\n",
    "di = get_dataset_info(dataset_name)\n",
    "train_dataset = dataset_registry.create(dataset_name, \"train\")\n",
    "\n",
    "test_dataset = dataset_registry.create(dataset_name, \"test\")\n",
    "cols = train_dataset.feature_columns + train_dataset.label_columns\n",
    "\n",
    "train_txt = train_dataset.data[cols]\n",
    "test_txt = test_dataset.data[cols]\n",
    "\n",
    "# load dataset from dataframe\n",
    "train_ds = Dataset.from_pandas(train_txt)\n",
    "train_ds = train_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "test_ds = Dataset.from_pandas(test_txt)\n",
    "test_ds = test_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "\n",
    "train_ds = train_ds.train_test_split(\n",
    "    test_size=0.15, seed=42, stratify_by_column=train_dataset.label_columns[0]\n",
    ")\n",
    "\n",
    "ds = DatasetDict(\n",
    "    {\"train\": train_ds[\"train\"], \"validation\": train_ds[\"test\"], \"test\": test_ds}\n",
    ")\n",
    "\n",
    "# Now we have made the split but still need to deal with missing values, and that depends on the column type\n",
    "\n",
    "# All as text\n",
    "train_all_text = ds[\"train\"].to_pandas()\n",
    "val_all_text = ds[\"validation\"].to_pandas()\n",
    "test_all_text = ds[\"test\"].to_pandas()\n",
    "\n",
    "train_all_text[train_dataset.feature_columns] = train_all_text[\n",
    "    train_dataset.feature_columns\n",
    "].astype(\"str\")\n",
    "val_all_text[train_dataset.feature_columns] = val_all_text[\n",
    "    train_dataset.feature_columns\n",
    "].astype(\"str\")\n",
    "test_all_text[train_dataset.feature_columns] = test_all_text[\n",
    "    train_dataset.feature_columns\n",
    "].astype(\"str\")\n",
    "\n",
    "ds_all_text = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(train_all_text),\n",
    "        \"validation\": Dataset.from_pandas(val_all_text),\n",
    "        \"test\": Dataset.from_pandas(test_all_text),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Dataset\n",
    "di = get_dataset_info(dataset_name, model_type=\"all_as_text\")\n",
    "# dataset = load_dataset(di.ds_name) # , download_mode=\"reuse_dataset_if_exists\")\n",
    "dataset = ds_all_text\n",
    "dataset = prepare_text(dataset, \"all_as_text\", dataset_name)\n",
    "if di.prob_type == \"regression\":\n",
    "    mean_price = np.mean(dataset[\"train\"][\"label\"])\n",
    "    std_price = np.std(dataset[\"train\"][\"label\"])\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_base, num_labels=di.num_labels, problem_type=di.prob_type\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_base)\n",
    "\n",
    "# Tokenize the dataset\n",
    "\n",
    "\n",
    "def encode(examples):\n",
    "    return {\n",
    "        \"label\": np.array([examples[di.label_col]]),\n",
    "        **tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            # max_length=args[\"max_length\"],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(encode, load_from_cache_file=False)\n",
    "\n",
    "# # Fast dev run if want to run quickly and not save to wandb\n",
    "# if args[\"fast_dev_run\"]:\n",
    "#     args[\"num_epochs\"] = 1\n",
    "#     args[\"tags\"].append(\"fast-dev-run\")\n",
    "#     dataset[\"train\"] = dataset[\"train\"].select(range(500))\n",
    "#     dataset[\"test\"] = dataset[\"test\"].select(range(10))\n",
    "#     output_dir = os.path.join(args[\"output_root\"], \"testing\")\n",
    "#     print(\n",
    "#         \"\\n######################    Running in fast dev mode    #######################\\n\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(i) for i in dataset[\"train\"][\"input_ids\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.882352941176471e-05"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([l for l in lens if l > 512]) / len(lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset airbnb, all as text version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--melbourne_airbnb_all_text-ca650dbf1ce581bf/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 372.46it/s]\n",
      "No model type specified for airbnb. (This is fine during dataset creation)\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map:   0%|          | 0/2748 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "model_base = \"distilroberta-base\"\n",
    "dataset_name = \"airbnb\"\n",
    "\n",
    "\n",
    "# di = get_dataset_info(dataset_name)\n",
    "# train_dataset = dataset_registry.create(dataset_name, \"train\")\n",
    "\n",
    "# test_dataset = dataset_registry.create(dataset_name, \"test\")\n",
    "# cols = train_dataset.feature_columns + train_dataset.label_columns\n",
    "\n",
    "# train_txt = train_dataset.data[cols]\n",
    "# test_txt = test_dataset.data[cols]\n",
    "\n",
    "# # load dataset from dataframe\n",
    "# train_ds = Dataset.from_pandas(train_txt)\n",
    "# train_ds = train_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "# test_ds = Dataset.from_pandas(test_txt)\n",
    "# test_ds = test_ds.class_encode_column(train_dataset.label_columns[0])\n",
    "\n",
    "# train_ds = train_ds.train_test_split(\n",
    "#     test_size=0.15, seed=42, stratify_by_column=train_dataset.label_columns[0]\n",
    "# )\n",
    "\n",
    "# ds = DatasetDict(\n",
    "#     {\"train\": train_ds[\"train\"], \"validation\": train_ds[\"test\"], \"test\": test_ds}\n",
    "# )\n",
    "\n",
    "# # Now we have made the split but still need to deal with missing values, and that depends on the column type\n",
    "\n",
    "# # All as text\n",
    "# train_all_text = ds[\"train\"].to_pandas()\n",
    "# val_all_text = ds[\"validation\"].to_pandas()\n",
    "# test_all_text = ds[\"test\"].to_pandas()\n",
    "\n",
    "# train_all_text[train_dataset.feature_columns] = train_all_text[\n",
    "#     train_dataset.feature_columns\n",
    "# ].astype(\"str\")\n",
    "# val_all_text[train_dataset.feature_columns] = val_all_text[\n",
    "#     train_dataset.feature_columns\n",
    "# ].astype(\"str\")\n",
    "# test_all_text[train_dataset.feature_columns] = test_all_text[\n",
    "#     train_dataset.feature_columns\n",
    "# ].astype(\"str\")\n",
    "\n",
    "# ds_all_text = DatasetDict(\n",
    "#     {\n",
    "#         \"train\": Dataset.from_pandas(train_all_text),\n",
    "#         \"validation\": Dataset.from_pandas(val_all_text),\n",
    "#         \"test\": Dataset.from_pandas(test_all_text),\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# Dataset\n",
    "di = get_dataset_info(dataset_name, model_type=\"all_as_text\")\n",
    "# , download_mode=\"reuse_dataset_if_exists\")\n",
    "dataset = load_dataset(di.ds_name)\n",
    "# dataset = ds_all_text\n",
    "dataset = prepare_text(dataset, \"all_as_text\", dataset_name)\n",
    "if di.prob_type == \"regression\":\n",
    "    mean_price = np.mean(dataset[\"train\"][\"label\"])\n",
    "    std_price = np.std(dataset[\"train\"][\"label\"])\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_base, num_labels=di.num_labels, problem_type=di.prob_type\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_base)\n",
    "\n",
    "# Tokenize the dataset\n",
    "\n",
    "\n",
    "def encode(examples):\n",
    "    return {\n",
    "        \"label\": np.array([examples[di.label_col]]),\n",
    "        **tokenizer(\n",
    "            examples[\"text\"],\n",
    "            # truncation=True,\n",
    "            # padding=\"max_length\",\n",
    "            # max_length=args[\"max_length\"],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(encode)  # , load_from_cache_file=False)\n",
    "\n",
    "# # Fast dev run if want to run quickly and not save to wandb\n",
    "# if args[\"fast_dev_run\"]:\n",
    "#     args[\"num_epochs\"] = 1\n",
    "#     args[\"tags\"].append(\"fast-dev-run\")\n",
    "#     dataset[\"train\"] = dataset[\"train\"].select(range(500))\n",
    "#     dataset[\"test\"] = dataset[\"test\"].select(range(10))\n",
    "#     output_dir = os.path.join(args[\"output_root\"], \"testing\")\n",
    "#     print(\n",
    "#         \"\\n######################    Running in fast dev mode    #######################\\n\"\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(i) for i in dataset[\"train\"][\"input_ids\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40364850976361766"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([l for l in lens if l > 512]) / len(lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[455,\n",
       " 479,\n",
       " 539,\n",
       " 429,\n",
       " 492,\n",
       " 430,\n",
       " 488,\n",
       " 532,\n",
       " 456,\n",
       " 501,\n",
       " 474,\n",
       " 586,\n",
       " 469,\n",
       " 443,\n",
       " 541,\n",
       " 469,\n",
       " 634,\n",
       " 493,\n",
       " 377,\n",
       " 498,\n",
       " 598,\n",
       " 581,\n",
       " 409,\n",
       " 507,\n",
       " 399,\n",
       " 537,\n",
       " 650,\n",
       " 489,\n",
       " 454,\n",
       " 455,\n",
       " 548,\n",
       " 546,\n",
       " 448,\n",
       " 579,\n",
       " 483,\n",
       " 377,\n",
       " 437,\n",
       " 366,\n",
       " 455,\n",
       " 703,\n",
       " 522,\n",
       " 404,\n",
       " 681,\n",
       " 509,\n",
       " 400,\n",
       " 521,\n",
       " 513,\n",
       " 807,\n",
       " 493,\n",
       " 530,\n",
       " 550,\n",
       " 500,\n",
       " 449,\n",
       " 556,\n",
       " 1341,\n",
       " 607,\n",
       " 394,\n",
       " 477,\n",
       " 474,\n",
       " 538,\n",
       " 518,\n",
       " 400,\n",
       " 631,\n",
       " 557,\n",
       " 484,\n",
       " 524,\n",
       " 476,\n",
       " 524,\n",
       " 476,\n",
       " 593,\n",
       " 469,\n",
       " 453,\n",
       " 477,\n",
       " 471,\n",
       " 480,\n",
       " 442,\n",
       " 592,\n",
       " 480,\n",
       " 474,\n",
       " 483,\n",
       " 536,\n",
       " 703,\n",
       " 575,\n",
       " 643,\n",
       " 521,\n",
       " 458,\n",
       " 485,\n",
       " 553,\n",
       " 479,\n",
       " 440,\n",
       " 555,\n",
       " 636,\n",
       " 541,\n",
       " 916,\n",
       " 500,\n",
       " 469,\n",
       " 466,\n",
       " 546,\n",
       " 601,\n",
       " 440,\n",
       " 544,\n",
       " 533,\n",
       " 406,\n",
       " 432,\n",
       " 548,\n",
       " 476,\n",
       " 351,\n",
       " 492,\n",
       " 430,\n",
       " 506,\n",
       " 428,\n",
       " 693,\n",
       " 529,\n",
       " 567,\n",
       " 458,\n",
       " 547,\n",
       " 593,\n",
       " 392,\n",
       " 549,\n",
       " 546,\n",
       " 380,\n",
       " 426,\n",
       " 527,\n",
       " 552,\n",
       " 438,\n",
       " 472,\n",
       " 407,\n",
       " 368,\n",
       " 447,\n",
       " 464,\n",
       " 432,\n",
       " 452,\n",
       " 445,\n",
       " 457,\n",
       " 423,\n",
       " 443,\n",
       " 489,\n",
       " 570,\n",
       " 349,\n",
       " 344,\n",
       " 487,\n",
       " 424,\n",
       " 446,\n",
       " 414,\n",
       " 445,\n",
       " 633,\n",
       " 495,\n",
       " 447,\n",
       " 442,\n",
       " 390,\n",
       " 347,\n",
       " 370,\n",
       " 491,\n",
       " 449,\n",
       " 419,\n",
       " 593,\n",
       " 502,\n",
       " 516,\n",
       " 461,\n",
       " 332,\n",
       " 534,\n",
       " 563,\n",
       " 611,\n",
       " 576,\n",
       " 581,\n",
       " 660,\n",
       " 596,\n",
       " 557,\n",
       " 453,\n",
       " 406,\n",
       " 516,\n",
       " 417,\n",
       " 650,\n",
       " 454,\n",
       " 413,\n",
       " 613,\n",
       " 367,\n",
       " 517,\n",
       " 550,\n",
       " 436,\n",
       " 500,\n",
       " 525,\n",
       " 750,\n",
       " 533,\n",
       " 632,\n",
       " 542,\n",
       " 529,\n",
       " 545,\n",
       " 602,\n",
       " 606,\n",
       " 453,\n",
       " 550,\n",
       " 597,\n",
       " 495,\n",
       " 519,\n",
       " 345,\n",
       " 483,\n",
       " 615,\n",
       " 518,\n",
       " 495,\n",
       " 526,\n",
       " 488,\n",
       " 649,\n",
       " 482,\n",
       " 587,\n",
       " 501,\n",
       " 496,\n",
       " 511,\n",
       " 563,\n",
       " 610,\n",
       " 469,\n",
       " 586,\n",
       " 381,\n",
       " 470,\n",
       " 486,\n",
       " 545,\n",
       " 423,\n",
       " 463,\n",
       " 547,\n",
       " 420,\n",
       " 495,\n",
       " 463,\n",
       " 429,\n",
       " 682,\n",
       " 499,\n",
       " 495,\n",
       " 561,\n",
       " 513,\n",
       " 415,\n",
       " 430,\n",
       " 537,\n",
       " 379,\n",
       " 462,\n",
       " 663,\n",
       " 1097,\n",
       " 536,\n",
       " 462,\n",
       " 682,\n",
       " 507,\n",
       " 547,\n",
       " 506,\n",
       " 483,\n",
       " 553,\n",
       " 499,\n",
       " 390,\n",
       " 529,\n",
       " 413,\n",
       " 444,\n",
       " 567,\n",
       " 483,\n",
       " 493,\n",
       " 550,\n",
       " 417,\n",
       " 580,\n",
       " 482,\n",
       " 635,\n",
       " 596,\n",
       " 478,\n",
       " 553,\n",
       " 581,\n",
       " 449,\n",
       " 529,\n",
       " 417,\n",
       " 512,\n",
       " 501,\n",
       " 515,\n",
       " 679,\n",
       " 445,\n",
       " 458,\n",
       " 603,\n",
       " 436,\n",
       " 625,\n",
       " 678,\n",
       " 385,\n",
       " 504,\n",
       " 499,\n",
       " 531,\n",
       " 1053,\n",
       " 410,\n",
       " 703,\n",
       " 792,\n",
       " 537,\n",
       " 541,\n",
       " 490,\n",
       " 525,\n",
       " 473,\n",
       " 561,\n",
       " 484,\n",
       " 512,\n",
       " 501,\n",
       " 738,\n",
       " 465,\n",
       " 322,\n",
       " 367,\n",
       " 518,\n",
       " 390,\n",
       " 466,\n",
       " 647,\n",
       " 459,\n",
       " 547,\n",
       " 454,\n",
       " 632,\n",
       " 475,\n",
       " 492,\n",
       " 836,\n",
       " 661,\n",
       " 537,\n",
       " 623,\n",
       " 388,\n",
       " 455,\n",
       " 517,\n",
       " 451,\n",
       " 414,\n",
       " 545,\n",
       " 459,\n",
       " 638,\n",
       " 550,\n",
       " 358,\n",
       " 387,\n",
       " 451,\n",
       " 507,\n",
       " 467,\n",
       " 478,\n",
       " 572,\n",
       " 479,\n",
       " 416,\n",
       " 536,\n",
       " 360,\n",
       " 391,\n",
       " 637,\n",
       " 482,\n",
       " 620,\n",
       " 407,\n",
       " 400,\n",
       " 466,\n",
       " 412,\n",
       " 492,\n",
       " 461,\n",
       " 503,\n",
       " 582,\n",
       " 596,\n",
       " 541,\n",
       " 462,\n",
       " 417,\n",
       " 441,\n",
       " 655,\n",
       " 460,\n",
       " 393,\n",
       " 481,\n",
       " 519,\n",
       " 616,\n",
       " 700,\n",
       " 440,\n",
       " 576,\n",
       " 537,\n",
       " 392,\n",
       " 412,\n",
       " 450,\n",
       " 454,\n",
       " 458,\n",
       " 496,\n",
       " 492,\n",
       " 629,\n",
       " 530,\n",
       " 486,\n",
       " 554,\n",
       " 423,\n",
       " 526,\n",
       " 490,\n",
       " 600,\n",
       " 496,\n",
       " 403,\n",
       " 458,\n",
       " 657,\n",
       " 440,\n",
       " 509,\n",
       " 455,\n",
       " 464,\n",
       " 644,\n",
       " 640,\n",
       " 557,\n",
       " 579,\n",
       " 505,\n",
       " 505,\n",
       " 542,\n",
       " 508,\n",
       " 389,\n",
       " 435,\n",
       " 528,\n",
       " 426,\n",
       " 448,\n",
       " 351,\n",
       " 377,\n",
       " 573,\n",
       " 495,\n",
       " 694,\n",
       " 1286,\n",
       " 541,\n",
       " 589,\n",
       " 506,\n",
       " 540,\n",
       " 447,\n",
       " 554,\n",
       " 404,\n",
       " 586,\n",
       " 496,\n",
       " 359,\n",
       " 541,\n",
       " 460,\n",
       " 491,\n",
       " 323,\n",
       " 472,\n",
       " 489,\n",
       " 589,\n",
       " 384,\n",
       " 582,\n",
       " 511,\n",
       " 556,\n",
       " 675,\n",
       " 339,\n",
       " 382,\n",
       " 600,\n",
       " 530,\n",
       " 466,\n",
       " 426,\n",
       " 624,\n",
       " 527,\n",
       " 588,\n",
       " 481,\n",
       " 518,\n",
       " 440,\n",
       " 360,\n",
       " 401,\n",
       " 460,\n",
       " 480,\n",
       " 573,\n",
       " 464,\n",
       " 682,\n",
       " 562,\n",
       " 555,\n",
       " 462,\n",
       " 451,\n",
       " 511,\n",
       " 451,\n",
       " 528,\n",
       " 583,\n",
       " 471,\n",
       " 473,\n",
       " 570,\n",
       " 457,\n",
       " 634,\n",
       " 554,\n",
       " 414,\n",
       " 692,\n",
       " 546,\n",
       " 575,\n",
       " 426,\n",
       " 678,\n",
       " 483,\n",
       " 473,\n",
       " 425,\n",
       " 579,\n",
       " 525,\n",
       " 531,\n",
       " 614,\n",
       " 457,\n",
       " 518,\n",
       " 499,\n",
       " 398,\n",
       " 620,\n",
       " 629,\n",
       " 643,\n",
       " 459,\n",
       " 483,\n",
       " 467,\n",
       " 540,\n",
       " 538,\n",
       " 631,\n",
       " 449,\n",
       " 438,\n",
       " 451,\n",
       " 507,\n",
       " 556,\n",
       " 455,\n",
       " 534,\n",
       " 571,\n",
       " 394,\n",
       " 471,\n",
       " 417,\n",
       " 632,\n",
       " 488,\n",
       " 470,\n",
       " 472,\n",
       " 526,\n",
       " 471,\n",
       " 363,\n",
       " 432,\n",
       " 522,\n",
       " 514,\n",
       " 444,\n",
       " 513,\n",
       " 475,\n",
       " 541,\n",
       " 507,\n",
       " 558,\n",
       " 394,\n",
       " 433,\n",
       " 508,\n",
       " 561,\n",
       " 506,\n",
       " 435,\n",
       " 523,\n",
       " 426,\n",
       " 451,\n",
       " 423,\n",
       " 584,\n",
       " 560,\n",
       " 515,\n",
       " 485,\n",
       " 484,\n",
       " 548,\n",
       " 466,\n",
       " 478,\n",
       " 524,\n",
       " 488,\n",
       " 545,\n",
       " 479,\n",
       " 486,\n",
       " 405,\n",
       " 472,\n",
       " 402,\n",
       " 510,\n",
       " 475,\n",
       " 483,\n",
       " 502,\n",
       " 450,\n",
       " 498,\n",
       " 714,\n",
       " 486,\n",
       " 349,\n",
       " 382,\n",
       " 546,\n",
       " 452,\n",
       " 391,\n",
       " 368,\n",
       " 411,\n",
       " 436,\n",
       " 487,\n",
       " 463,\n",
       " 469,\n",
       " 424,\n",
       " 587,\n",
       " 463,\n",
       " 491,\n",
       " 458,\n",
       " 368,\n",
       " 614,\n",
       " 525,\n",
       " 411,\n",
       " 885,\n",
       " 755,\n",
       " 382,\n",
       " 459,\n",
       " 525,\n",
       " 462,\n",
       " 510,\n",
       " 523,\n",
       " 614,\n",
       " 649,\n",
       " 499,\n",
       " 603,\n",
       " 491,\n",
       " 554,\n",
       " 605,\n",
       " 457,\n",
       " 544,\n",
       " 443,\n",
       " 499,\n",
       " 526,\n",
       " 510,\n",
       " 464,\n",
       " 440,\n",
       " 468,\n",
       " 536,\n",
       " 633,\n",
       " 599,\n",
       " 358,\n",
       " 531,\n",
       " 500,\n",
       " 528,\n",
       " 456,\n",
       " 491,\n",
       " 552,\n",
       " 520,\n",
       " 594,\n",
       " 622,\n",
       " 420,\n",
       " 384,\n",
       " 548,\n",
       " 572,\n",
       " 568,\n",
       " 544,\n",
       " 600,\n",
       " 472,\n",
       " 496,\n",
       " 415,\n",
       " 518,\n",
       " 491,\n",
       " 666,\n",
       " 506,\n",
       " 409,\n",
       " 497,\n",
       " 531,\n",
       " 641,\n",
       " 559,\n",
       " 461,\n",
       " 446,\n",
       " 591,\n",
       " 515,\n",
       " 543,\n",
       " 509,\n",
       " 459,\n",
       " 656,\n",
       " 378,\n",
       " 375,\n",
       " 454,\n",
       " 445,\n",
       " 391,\n",
       " 521,\n",
       " 510,\n",
       " 453,\n",
       " 524,\n",
       " 520,\n",
       " 531,\n",
       " 449,\n",
       " 541,\n",
       " 586,\n",
       " 474,\n",
       " 501,\n",
       " 507,\n",
       " 501,\n",
       " 424,\n",
       " 389,\n",
       " 439,\n",
       " 493,\n",
       " 462,\n",
       " 517,\n",
       " 398,\n",
       " 458,\n",
       " 538,\n",
       " 530,\n",
       " 543,\n",
       " 472,\n",
       " 498,\n",
       " 396,\n",
       " 448,\n",
       " 452,\n",
       " 429,\n",
       " 563,\n",
       " 411,\n",
       " 404,\n",
       " 459,\n",
       " 407,\n",
       " 449,\n",
       " 526,\n",
       " 339,\n",
       " 548,\n",
       " 516,\n",
       " 502,\n",
       " 586,\n",
       " 633,\n",
       " 413,\n",
       " 466,\n",
       " 439,\n",
       " 523,\n",
       " 527,\n",
       " 626,\n",
       " 512,\n",
       " 653,\n",
       " 483,\n",
       " 766,\n",
       " 439,\n",
       " 596,\n",
       " 631,\n",
       " 464,\n",
       " 384,\n",
       " 593,\n",
       " 451,\n",
       " 425,\n",
       " 514,\n",
       " 531,\n",
       " 582,\n",
       " 541,\n",
       " 694,\n",
       " 549,\n",
       " 570,\n",
       " 454,\n",
       " 753,\n",
       " 577,\n",
       " 428,\n",
       " 518,\n",
       " 423,\n",
       " 481,\n",
       " 535,\n",
       " 547,\n",
       " 479,\n",
       " 402,\n",
       " 516,\n",
       " 583,\n",
       " 612,\n",
       " 466,\n",
       " 517,\n",
       " 534,\n",
       " 554,\n",
       " 443,\n",
       " 507,\n",
       " 569,\n",
       " 539,\n",
       " 566,\n",
       " 545,\n",
       " 718,\n",
       " 424,\n",
       " 895,\n",
       " 512,\n",
       " 407,\n",
       " 632,\n",
       " 355,\n",
       " 480,\n",
       " 402,\n",
       " 494,\n",
       " 487,\n",
       " 444,\n",
       " 481,\n",
       " 476,\n",
       " 387,\n",
       " 464,\n",
       " 547,\n",
       " 490,\n",
       " 575,\n",
       " 536,\n",
       " 463,\n",
       " 492,\n",
       " 402,\n",
       " 474,\n",
       " 628,\n",
       " 463,\n",
       " 571,\n",
       " 668,\n",
       " 422,\n",
       " 666,\n",
       " 486,\n",
       " 448,\n",
       " 531,\n",
       " 638,\n",
       " 489,\n",
       " 492,\n",
       " 523,\n",
       " 459,\n",
       " 377,\n",
       " 494,\n",
       " 429,\n",
       " 529,\n",
       " 439,\n",
       " 368,\n",
       " 534,\n",
       " 547,\n",
       " 445,\n",
       " 553,\n",
       " 458,\n",
       " 508,\n",
       " 349,\n",
       " 622,\n",
       " 550,\n",
       " 650,\n",
       " 527,\n",
       " 383,\n",
       " 482,\n",
       " 409,\n",
       " 542,\n",
       " 430,\n",
       " 595,\n",
       " 552,\n",
       " 544,\n",
       " 465,\n",
       " 479,\n",
       " 529,\n",
       " 370,\n",
       " 418,\n",
       " 526,\n",
       " 640,\n",
       " 441,\n",
       " 529,\n",
       " 425,\n",
       " 535,\n",
       " 456,\n",
       " 295,\n",
       " 462,\n",
       " 541,\n",
       " 523,\n",
       " 576,\n",
       " 521,\n",
       " 552,\n",
       " 464,\n",
       " 519,\n",
       " 435,\n",
       " 596,\n",
       " 428,\n",
       " 591,\n",
       " 560,\n",
       " 370,\n",
       " 459,\n",
       " 468,\n",
       " 509,\n",
       " 559,\n",
       " 566,\n",
       " 441,\n",
       " 432,\n",
       " 383,\n",
       " 529,\n",
       " 450,\n",
       " 588,\n",
       " 490,\n",
       " 408,\n",
       " 563,\n",
       " 446,\n",
       " 427,\n",
       " 471,\n",
       " 401,\n",
       " 376,\n",
       " 598,\n",
       " 645,\n",
       " 554,\n",
       " 468,\n",
       " 425,\n",
       " 673,\n",
       " 448,\n",
       " 548,\n",
       " 452,\n",
       " 565,\n",
       " 887,\n",
       " 431,\n",
       " 384,\n",
       " 557,\n",
       " 538,\n",
       " 535,\n",
       " 423,\n",
       " 472,\n",
       " 485,\n",
       " 515,\n",
       " 422,\n",
       " 478,\n",
       " 581,\n",
       " 584,\n",
       " 550,\n",
       " 404,\n",
       " 410,\n",
       " 421,\n",
       " 574,\n",
       " 490,\n",
       " 489,\n",
       " 475,\n",
       " 491,\n",
       " 603,\n",
       " 497,\n",
       " 455,\n",
       " 565,\n",
       " 509,\n",
       " 667,\n",
       " 475,\n",
       " 398,\n",
       " 505,\n",
       " 473,\n",
       " 420,\n",
       " 456,\n",
       " 609,\n",
       " 558,\n",
       " 384,\n",
       " 417,\n",
       " 521,\n",
       " 506,\n",
       " 458,\n",
       " 532,\n",
       " 572,\n",
       " 389,\n",
       " 509,\n",
       " 348,\n",
       " 527,\n",
       " 496,\n",
       " 490,\n",
       " 483,\n",
       " 459,\n",
       " 349,\n",
       " 479,\n",
       " 430,\n",
       " 619,\n",
       " 502,\n",
       " 473,\n",
       " 705,\n",
       " 637,\n",
       " 501,\n",
       " 834,\n",
       " 475,\n",
       " 472,\n",
       " 581,\n",
       " 544,\n",
       " 426,\n",
       " 475,\n",
       " 409,\n",
       " 494,\n",
       " 394,\n",
       " 474,\n",
       " 528,\n",
       " 507,\n",
       " 360,\n",
       " 644,\n",
       " 446,\n",
       " 427,\n",
       " 503,\n",
       " 495,\n",
       " 514,\n",
       " 387,\n",
       " 576,\n",
       " 549,\n",
       " 747,\n",
       " 485,\n",
       " 548,\n",
       " 404,\n",
       " 792,\n",
       " 617,\n",
       " 558,\n",
       " 418,\n",
       " 372,\n",
       " 426,\n",
       " 385,\n",
       " 469,\n",
       " 428,\n",
       " 467,\n",
       " 395,\n",
       " 509,\n",
       " 674,\n",
       " 489,\n",
       " 609,\n",
       " 590,\n",
       " 632,\n",
       " 448,\n",
       " 456,\n",
       " 444,\n",
       " 407,\n",
       " 536,\n",
       " 456,\n",
       " 613,\n",
       " 507,\n",
       " 447,\n",
       " 463,\n",
       " 414,\n",
       " 546,\n",
       " 427,\n",
       " 445,\n",
       " 515,\n",
       " 532,\n",
       " 579,\n",
       " 593,\n",
       " 1227,\n",
       " 353,\n",
       " 573,\n",
       " 480,\n",
       " 407,\n",
       " 529,\n",
       " 602,\n",
       " 383,\n",
       " 486,\n",
       " 509,\n",
       " 417,\n",
       " 379,\n",
       " 565,\n",
       " 464,\n",
       " 488,\n",
       " 557,\n",
       " 417,\n",
       " 505,\n",
       " 490,\n",
       " 512,\n",
       " 529,\n",
       " 458,\n",
       " 562,\n",
       " 584,\n",
       " 517,\n",
       " 499,\n",
       " 331,\n",
       " 531,\n",
       " 414,\n",
       " 421,\n",
       " 446,\n",
       " 396,\n",
       " 450,\n",
       " 554,\n",
       " 415,\n",
       " 414,\n",
       " 489,\n",
       " 577,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
